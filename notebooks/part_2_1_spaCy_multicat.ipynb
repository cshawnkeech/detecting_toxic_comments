{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaWoMJGfVwUB"
   },
   "source": [
    "# Detecting and Classifying Toxic Comments\n",
    "# Part 2-1: spaCy Multi-Category Test\n",
    "\n",
    "We'll train a spaCy model to categorize multiple, non-mutually exclusive categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDRcsr5AZfQb",
    "tags": []
   },
   "source": [
    "## Python Library Imports\n",
    "\n",
    "\n",
    "Resources:\n",
    "- [pool]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24997,
     "status": "ok",
     "timestamp": 1616376286409,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "mQY7o6xDZhTe",
    "outputId": "73aee6d3-7edd-437f-eab4-36c67320b567",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "\n",
    "# scikit learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTRgdYMCHaHD",
    "tags": []
   },
   "source": [
    "## spaCy Setup & Imports\n",
    "\n",
    "As mentioned previously, we'll be using spaCy version 2.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    2.3.5                         \n",
      "Location         /opt/anaconda3/lib/python3.7/site-packages/spacy\n",
      "Platform         Darwin-20.3.0-x86_64-i386-64bit\n",
      "Python version   3.7.6                         \n",
      "Models                                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # check version\n",
    "! python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy Imports\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "from spacy.scorer import Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# add src folder to path\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "# from text_prep import tidy_series, uppercase_proportion_column\n",
    "from spacy_helper import doc_check, add_labels_helper, txt_and_multi_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from toxic_basic Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72.5 ms, sys: 55.7 ms, total: 128 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "last load time:\n",
    "\n",
    "CPU times: user 67 ms, sys: 46.7 ms, total: 114 ms\n",
    "Wall time: 114 ms\n",
    "'''\n",
    "\n",
    "# load toxic_basic pickle into dataframe\n",
    "path_toxic_basic = \"../data/toxic_basic.pkl\"\n",
    "\n",
    "toxic_df = pd.read_pickle(path_toxic_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   comment_text          159571 non-null  object \n",
      " 1   uppercase_proportion  159548 non-null  float64\n",
      " 2   toxic                 159571 non-null  int64  \n",
      " 3   severe_toxic          159571 non-null  int64  \n",
      " 4   obscene               159571 non-null  int64  \n",
      " 5   threat                159571 non-null  int64  \n",
      " 6   insult                159571 non-null  int64  \n",
      " 7   identity_hate         159571 non-null  int64  \n",
      "dtypes: float64(1), int64(6), object(1)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_qN8SvmysyP"
   },
   "source": [
    "# Stratified Train Test Split\n",
    "\n",
    "As our process should first determine whether the text is toxic or not toxic, we'll make a simplified stratified train test split, ensuring our balance of toxic and non-toxic rows are proportionally distributed.\n",
    "\n",
    "For now, we won't be too concerned with the proportion of sub-categories, just the primary categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTncCacxukC-"
   },
   "source": [
    "## Stratified Split maintaining ratio of toxic to not toxic texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_text', 'uppercase_proportion', 'toxic', 'severe_toxic',\n",
       "       'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check current columns\n",
    "toxic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "aborted",
     "timestamp": 1616376571555,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "-3rJS3ZRuipN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X columns: Index(['comment_text', 'uppercase_proportion'], dtype='object')\n",
      "y columns:Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
      "       'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# split df into X(independent) and y(depenendent) groups\n",
    "ind_cols = ['comment_text', 'uppercase_proportion']\n",
    "\n",
    "X = toxic_df[ind_cols]\n",
    "y = toxic_df.drop(columns=ind_cols)\n",
    "\n",
    "print(f\"X columns: {X.columns}\\ny columns:{y.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "aborted",
     "timestamp": 1616376571556,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "WJZnb8Pl1Stk"
   },
   "outputs": [],
   "source": [
    "# Train Test Split. Stratified on y['toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y['toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preserve Test Train Split Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls ../data/basic_df_split/\n",
    "\n",
    "X_train.to_pickle('../data/basic_df_split/basic_X_train.pkl')\n",
    "X_test.to_pickle('../data/basic_df_split/basic_X_test.pkl')\n",
    "y_train.to_pickle('../data/basic_df_split/basic_y_train.pkl')\n",
    "y_test.to_pickle('../data/basic_df_split/basic_y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egZZdQA3hS2u"
   },
   "source": [
    "# spaCy\n",
    "\n",
    "Let's try out spaCy, a nlp processing library!\n",
    "\n",
    "- https://course.spacy.io/en/chapter1\n",
    "- [text classification with spaCy](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/) \n",
    "- [customized list of stopwords](https://spacy.io/usage/linguistic-features#stop-words)  \n",
    "- [Split Series into list of sentences](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.cat.html)  \n",
    "- [contractions](https://theslaps.medium.com/cant-stand-don-t-want-contractions-with-spacy-39715cac2ebb)  \n",
    "\n",
    "\n",
    "- [v2.spacy.io](https://v2.spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train spaCy Model for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4D7rYTjHXYY",
    "tags": []
   },
   "source": [
    "## Establish spaCy Pipeline\n",
    "\n",
    "\"spaCy's components are supervised models for text annotations, meaning that they can only learn to reproduce examples, not guess new labels from raw text.\"\n",
    "\n",
    "By default, spaCy's text categorizer is a simple convolutional neural network.\n",
    "\n",
    "Resources:\n",
    "- [for emojis](https://spacy.io/universe/project/spacymoji)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is modified from tutorial here:\n",
    "\n",
    "Resource:\n",
    "https://www.machinelearningplus.com/nlp/custom-text-classification-spacy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "- [spaCy docs: scorer](https://spacy.io/api/scorer)  \n",
    "\n",
    "- [F-Score](https://en.wikipedia.org/wiki/F-score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## if the model is not yet locally available\n",
    "# ! python -m spacy download en_core_web_lg\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "# Provide scoring pipeline\n",
    "scorer = Scorer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcat = nlp.create_pipe('textcat')\n",
    "\n",
    "nlp.add_pipe(textcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner', 'textcat']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "severe_toxic\n",
      "obscene\n",
      "threat\n",
      "insult\n",
      "identity_hate\n"
     ]
    }
   ],
   "source": [
    "# use custom function to add labels to textcat\n",
    "add_labels_helper(y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TOXIC', 'SEVERE_TOXIC', 'OBSCENE', 'THREAT', 'INSULT', 'IDENTITY_HATE')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcat.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[zipping from dict of unknown size](https://stackoverflow.com/a/40658867)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TOXIC', 'SEVERE_TOXIC', 'OBSCENE', 'THREAT', 'INSULT', 'IDENTITY_HATE']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"' Meša Selimović I'm not opposing such a formulation. Instead I'm trying to prevent potential editwars by inclusion of both views (Bosnian and Serbian writer). However, it seems that there is a third view (Yugoslavian writer) and so an option is to mention all of them (Yugoslavian, Bosnian and Serbian writer). Although this may read cumbersome, maybe it's worth trying. All the best. 's talk '\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"' September 2008 (UTC) Talking about how he was victimized because of the release of his name is an implication that releasing his name was bad. (talk | contribs) 03:17, 9'\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (', 1 August 2012 (UTC) Danke, - supports what we found, no need to ask further, - the Main page appearance on 26 July opened new heights, 08:05',\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"' The Aurora name One of the parts of the story that seemed particularly weak was the name. 'Aurora' was selected because it appeared as a line item in a budget document. I am currently reading a National Academy of Sciences report called 'Review of the Department of Energy’s Inertial Confinement Fusion Program', which was published in September 1990. The paper discusses a variety of issues relating to controlled fusion experiments, and led indirectly to the current National Ignition Facility efforts. Why do I mention this? Well on the very first page I found: ...and a subsequent meeting was held in La Jolla on August 21, 1990, to consider future experiments possible with the AURORA facility. Aurora was a high-power KrF ICF laser built at Los Alamos. Given the time frame, the general secrecy surrounding these machines, and the common name, is there any chance the line item in question was the budget for this laser? '\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"' WP:NOT: W is not a 'how to' Wikipedia is not a 'how to' manual. The text should not being giving out instructions about what to do when one finds oneself having symptoms of a disease or having a diagnosis of a disease. Furthermore, Wikipedia should not be givng out any medical advice at all. Please avoid phrases like 'women should...'. If a FACT is represented in some sentence that starts with 'Women should...', then please rephrase it into an objective FACT. Only licensed physicians should be saying what 'women should' do, and on some other site: NOT THIS ONE. '\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"Apology accepted, ! Mistakes happen, and in the scheme of things this one was sorted out pretty quickly, so don't feel too bad. Thanks for the apology though! Cheers, (let's chat)\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"' And you could pick any number from here . For example, 'of all the scientific journals, New Scientist has undoubtedly been the most supportive Of Sheldrake, having published a number of sympathetic articles on formative causation over the years.' '\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"' Should this be added? Should it be added on this page or the Great Britain page or both? -TC '\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"Note that according to Wikipedia's guideline on Date formatting and linking, don't link isolated years, so edits like this at Baby Phat are not necessary. Gimmetrow\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"' Hello, to begin with, nobody 'works' for Wikipedia as it is a volunteer project, but I am an adminstrator for the project. The problem that we have with the addition of text from non-free sources even by the copyright holder us that it is difficult to determine whether the person adding the text is who they claim to be or is the original author of the text; a further issue is that all edits to Wikipedia must by freely licensed and few commercial or governmental organisations are willing to do this. See, for example, your own assertion of copyright. Please also bear in mind that Wikipedia is an encyclopedia and that our content should be written with a neutral, dispassionate and scholarly tone. Text originally written for corporate promotion almost always needs to be rewritten to be appropriate for an encyclopedia. '\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " ('Multiple versions of #11 released from diversity of sources   Via Archive.org',\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " ('Plese sig your statements, if possible. Martial Law',\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"' I don't see any major differences between Israel and Sri Lanka to allow Sri Lanka's inclusion while excluding Israel. A couple of users have stated that Boyle is not credible so let's move on from him. The rest of the sources cited in the article are reporting claims made by fringe groups the Tamil nationalists in the diaspora. They are not 'Mainstream observers of world affairs' and are very much of the fringe, the UN and other have simply called the events in Sri Lanka war crimes and not genocide. Thus there is no basis for inclusion on this page. '\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"The article right now has it that Ami was supposed to unhuman in some way and that Osabu talked her out of it; we're just missing the specific way, which is I think that she was supposed to be a cyborg. Hence all the computer-related accessories. It's interesting, too, 'cause that same interest in having a cute cyborg girl got passed on to Hotaru eventually. ♫\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " ('Artkos, u should stop nagging. U are like the proverbial creacked record. Maybe if you got a life other than here?',\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"Planning to make comback for Quagga Well it's probably not hard to believe but they're planning to bring back the quagga through cloning. Some DNA samples that were left over in some hair samples of dead quaggas from the few musuems that have some stuffed ones. My professor is really into these reintroduction programs of extinct animals so he told me about it of coarse they won't tell the public about this out loud. What do ya'll think? mcelite\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " (\"' Fomenko's 'theory' is as FRINGE as it gets, and will not be treated as if it is a credible historical theory, because it isn't. So do not expect what you consider 'neutrality' in this article. You won't see Young earth creationism given credibility on the Age of the Earth article, so don't expect this one to mislead the reader to think that Fomenko's ideas are in any way legitimate. '\",\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " ('Regis & Kelly I SURE AM GOING TO MISS YOU REGIS. CAN YOU DO ME A FAVOR ON THE AIR 1 DAY, WILL YOU KISS KELLY RIPPA ON THE MOUTH WITH A GREAT BIG KISS. I AM SURE THAT YOUR WIFE WILL NOT MIND. I REALLY WISH THAT I COULD COME & MEET, SEE YOU IN PERSON BEFORE YOU RETIRE ON NOV 18TH, 2011. TAKE GOOD CARE OF YOURSELF REGIS. EVERYONE WILL BE THINKING OFYOU EVERDAY WHEN THE SHOW IS ON. LOTS OF LUCK REGIS',\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " ('Youre a sock puppet you are! you are! im going to track you down, and when i track you down, your going to drown, in your own sins. You are going to hell! I want to hell you! This is probably that pollypocket kid righting this by the way! yeah! so block the witch!',\n",
       "  {'cats': {'TOXIC': 1,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 1,\n",
       "    'INSULT': 1,\n",
       "    'IDENTITY_HATE': 0}}),\n",
       " ('y am i writing this?!??',\n",
       "  {'cats': {'TOXIC': 0,\n",
       "    'SEVERE_TOXIC': 0,\n",
       "    'OBSCENE': 0,\n",
       "    'THREAT': 0,\n",
       "    'INSULT': 0,\n",
       "    'IDENTITY_HATE': 0}})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = 20\n",
    "tiny_X = X_train['comment_text'][:tot]\n",
    "tiny_y = y_train[:tot]\n",
    "\n",
    "txt_and_multi_cat(tiny_X, tiny_y)\n",
    "# print(tiny_X, tiny_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Article for v3](https://medium.com/analytics-vidhya/building-a-text-classifier-with-spacy-3-0-dd16e9979a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! ls ../models/base_config.cfg\n",
    "# ! python -m spacy init fill-config ../models/base_config.cfg config.cfg --diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- X columns: Index(['comment_text', 'uppercase_proportion'], dtype='object')\n",
    "y columns:Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate',\n",
    "       'tuples'],\n",
    "      dtype='object') -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TOXIC', 'SEVERE_TOXIC', 'OBSCENE', 'THREAT', 'INSULT', 'IDENTITY_HATE']\n",
      "['TOXIC', 'SEVERE_TOXIC', 'OBSCENE', 'THREAT', 'INSULT', 'IDENTITY_HATE']\n"
     ]
    }
   ],
   "source": [
    "# formatting list of tuples for spacy training\n",
    "train_txt = X_train['comment_text']\n",
    "train_cat = y_train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "train_docs = txt_and_multi_cat(train_txt, train_cat)\n",
    "\n",
    "test_txt = X_test['comment_text']\n",
    "test_cat = y_test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "test_docs = txt_and_multi_cat(test_txt, test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Meša Selimović I'm not oppos, {'cats': {'TOXIC': 0, 'SEVERE_TOXIC': 0, 'OBSCENE': 0, 'THREAT': 0, 'INSULT': 0, 'IDENTITY_HATE': 0}}\n",
      "' September 2008 (UTC) Talking, {'cats': {'TOXIC': 0, 'SEVERE_TOXIC': 0, 'OBSCENE': 0, 'THREAT': 0, 'INSULT': 0, 'IDENTITY_HATE': 0}}\n",
      ", 1 August 2012 (UTC) Danke, -, {'cats': {'TOXIC': 0, 'SEVERE_TOXIC': 0, 'OBSCENE': 0, 'THREAT': 0, 'INSULT': 0, 'IDENTITY_HATE': 0}}\n",
      "' The Aurora name One of the p, {'cats': {'TOXIC': 0, 'SEVERE_TOXIC': 0, 'OBSCENE': 0, 'THREAT': 0, 'INSULT': 0, 'IDENTITY_HATE': 0}}\n",
      "' WP:NOT: W is not a 'how to' , {'cats': {'TOXIC': 0, 'SEVERE_TOXIC': 0, 'OBSCENE': 0, 'THREAT': 0, 'INSULT': 0, 'IDENTITY_HATE': 0}}\n"
     ]
    }
   ],
   "source": [
    "# this should be the correct format expected by the trainer\n",
    "\n",
    "# print(train_docs[0][1])\n",
    "first_five = [i for i in train_docs[:5]]\n",
    "\n",
    "for i in first_five:\n",
    "    print(f\"{i[0][:30]}, {i[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/custom-text-classification-spacy\n",
    "\n",
    "# Evaluate Function\n",
    "\n",
    "The evaluate function preserves the scores for each epoch. Scores are broken down categorically, and will assist us in checking the model's effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, val_texts, val_cats, thresh=0.5):\n",
    "    \n",
    "    docs = (tokenizer(val_text) for val_text in val_texts)  \n",
    "    \n",
    "    # create dict of results\n",
    "    evals_by_cat = dict()\n",
    "    \n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        \n",
    "        # actual scores \n",
    "        gold = val_cats[i]['cats']\n",
    "        \n",
    "        for label, score in doc.cats.items():\n",
    "            \n",
    "            # add label to dict if not already present\n",
    "            if label not in evals_by_cat:\n",
    "                evals_by_cat[label] = {'tp':0,\n",
    "                                       'fp':0,\n",
    "                                       'fn':0,\n",
    "                                       'tn':0,}\n",
    "            \n",
    "            if score >= thresh and gold[label] >= thresh:\n",
    "                evals_by_cat[label]['tp'] += 1\n",
    "\n",
    "            elif score >= thresh and gold[label] < thresh:\n",
    "                evals_by_cat[label]['fp'] += 1\n",
    "\n",
    "            elif score < thresh and gold[label] < thresh:\n",
    "                evals_by_cat[label]['tn'] += 1\n",
    "            \n",
    "            elif score < thresh and gold[label] >= thresh:\n",
    "                evals_by_cat[label]['fn'] += 1\n",
    "    \n",
    "    for key in evals_by_cat.keys():\n",
    "\n",
    "        # local scope variables to ease reading & debugging\n",
    "        tp = evals_by_cat[key]['tp']\n",
    "        fp = evals_by_cat[key]['fp']\n",
    "        fn = evals_by_cat[key]['fn']\n",
    "        tn = evals_by_cat[key]['tn']\n",
    "        \n",
    "        # precision\n",
    "        # edge case: avoid dividing by zero: precision = 1 when fp = 0\n",
    "        if tp + fp == 0:\n",
    "            evals_by_cat[key]['precision'] = 1\n",
    "        else:    \n",
    "            evals_by_cat[key]['precision'] = tp / (tp + fp)\n",
    "        \n",
    "        # recall\n",
    "        # edge case: avoid dividing by zero: recall = 1 when fn = 0\n",
    "        if tp + fn == 0:\n",
    "            evals_by_cat[key]['recall'] = 1\n",
    "        else:    \n",
    "            evals_by_cat[key]['recall'] = tp / (tp + fn)\n",
    "            \n",
    "        # local variables to ease reading & debugging\n",
    "        precision = evals_by_cat[key]['precision']\n",
    "        recall = evals_by_cat[key]['recall']\n",
    "        \n",
    "        # f score\n",
    "        if precision  + recall == 0:\n",
    "            evals_by_cat[key]['f_score'] = 0.0\n",
    "        else:\n",
    "            evals_by_cat[key]['f_score'] = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # preserve spaCy's native losses metric\n",
    "    evals_by_cat['TEXTCAT_LOSSES'] = losses['textcat']\n",
    "\n",
    "    return evals_by_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable used in testing\n",
    "# size = 1000\n",
    "train_data = train_docs\n",
    "dev_texts = [i[0] for i in test_docs]\n",
    "dev_cats = [i[1] for i in test_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do 5 Training Iterations \n",
    "\n",
    "We'll begin with 5 training iterations to help us estimate how long training will be and get a few baseline predictions.\n",
    "\n",
    "Each iteration takes roughly 12 minutes, so 5 will be about an hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## log from previous run:\n",
    "Training the model.\n",
    "Iterations: 5\n",
    "Timer Begins:14924.26\n",
    "epoch 1 start time: 14924.26\n",
    "textcat losses: 3.300574591791758\n",
    "epoch 1 end: 15645.511968644, epoch elapsed: 721.25\n",
    "\n",
    "epoch 2 start time: 15645.52\n",
    "textcat losses: 3.2344222257217226\n",
    "epoch 2 end: 16362.236106837, epoch elapsed: 716.72\n",
    "\n",
    "epoch 3 start time: 16362.24\n",
    "textcat losses: 2.8232882740304106\n",
    "epoch 3 end: 17072.387902298, epoch elapsed: 710.15\n",
    "\n",
    "epoch 4 start time: 17072.39\n",
    "textcat losses: 2.6497837365776666\n",
    "epoch 4 end: 17787.474139616, epoch elapsed: 715.08\n",
    "\n",
    "epoch 5 start time: 17787.48\n",
    "textcat losses: 2.3880158715215387\n",
    "epoch 5 end: 18499.152896641, epoch elapsed: 711.68\n",
    "\n",
    "Training complete. End:18499.16 Training Elapsed: 3574.90\n",
    "\n",
    "CPU times: user 1h 39min 33s, sys: 1h 3min 52s, total: 2h 43min 26s\n",
    "Wall time: 59min 34s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model.\n",
      "Iterations: 5\n",
      "Timer Begins:14924.26\n",
      "epoch 1 start time: 14924.26\n",
      "textcat losses: 3.300574591791758\n",
      "epoch 1 end: 15645.511968644, epoch elapsed: 721.25\n",
      "\n",
      "epoch 2 start time: 15645.52\n",
      "textcat losses: 3.2344222257217226\n",
      "epoch 2 end: 16362.236106837, epoch elapsed: 716.72\n",
      "\n",
      "epoch 3 start time: 16362.24\n",
      "textcat losses: 2.8232882740304106\n",
      "epoch 3 end: 17072.387902298, epoch elapsed: 710.15\n",
      "\n",
      "epoch 4 start time: 17072.39\n",
      "textcat losses: 2.6497837365776666\n",
      "epoch 4 end: 17787.474139616, epoch elapsed: 715.08\n",
      "\n",
      "epoch 5 start time: 17787.48\n",
      "textcat losses: 2.3880158715215387\n",
      "epoch 5 end: 18499.152896641, epoch elapsed: 711.68\n",
      "\n",
      "Training complete. End:18499.16 Training Elapsed: 3574.90\n",
      "\n",
      "CPU times: user 1h 39min 33s, sys: 1h 3min 52s, total: 2h 43min 26s\n",
      "Wall time: 59min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "log_list = list()\n",
    "\n",
    "# start = timer()\n",
    "# # ...\n",
    "# end = timer()\n",
    "# print(end - start) # Time in seconds, e.g. 5.38091952400282\n",
    "\n",
    "#(\"Number of training iterations\", \"n\", int))\n",
    "n_iter=5\n",
    "\n",
    "# Disabling other components\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  \n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    start_cumulative = timer()\n",
    "    print(f\"Training the model.\\nIterations: {n_iter}\\nTimer Begins:{start_cumulative:.2f}\")\n",
    "#     print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "\n",
    "    # Performing training\n",
    "    for i in range(n_iter):\n",
    "        start_epoch = timer()\n",
    "        print(f'epoch {i + 1} start time: {start_epoch:.2f}')\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        \n",
    "        for batch in batches:\n",
    "#             print(f\"new batch at :{timer():.2f}\")\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, \n",
    "                       annotations, \n",
    "                       sgd=optimizer, \n",
    "                       drop=0.2,\n",
    "                       losses=losses)\n",
    "\n",
    "      # Calling the evaluate() function and printing the scores\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            evals_by_cat = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            # reduced scores for testing\n",
    "            print(f'textcat losses:', losses['textcat'])\n",
    "            log_list.append(evals_by_cat)\n",
    "            \n",
    "            end_epoch = timer()\n",
    "            time.strftime(\"%Hh%Mm%Ss\", time.gmtime(4*3600+13*60+6)) \n",
    "            print(f'epoch {i + 1} end: {end_epoch}, epoch elapsed: {end_epoch-start_epoch:.2f}\\n')\n",
    "         \n",
    "            \n",
    "#             for key in evals_by_cat.keys():\n",
    "#                 print(f'{key}:\\n{evals_by_cat[key]}')\n",
    "#         print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  \n",
    "#               .format(losses['textcat'], scores['textcat_p'],\n",
    "#                       scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "end_cumulative = timer()\n",
    "print(f\"Training complete. End:{end_cumulative:.2f} Training Elapsed: {end_cumulative - start_cumulative:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(log_list)\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# with open('../logs/log_list_2.pkl', 'wb') as f:\n",
    "#     pickle.dump(log_list, f)\n",
    "\n",
    "# log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../logs/log_list_1.pkl', 'rb') as f:\n",
    "#     test_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TOXIC': {'tp': 3854,\n",
       "   'fp': 663,\n",
       "   'fn': 1193,\n",
       "   'tn': 46949,\n",
       "   'precision': 0.8532211644897055,\n",
       "   'recall': 0.7636219536358233,\n",
       "   'f_score': 0.8059389376829779},\n",
       "  'SEVERE_TOXIC': {'tp': 95,\n",
       "   'fp': 77,\n",
       "   'fn': 422,\n",
       "   'tn': 52065,\n",
       "   'precision': 0.5523255813953488,\n",
       "   'recall': 0.18375241779497098,\n",
       "   'f_score': 0.2757619738751814},\n",
       "  'OBSCENE': {'tp': 2211,\n",
       "   'fp': 409,\n",
       "   'fn': 551,\n",
       "   'tn': 49488,\n",
       "   'precision': 0.8438931297709924,\n",
       "   'recall': 0.8005068790731354,\n",
       "   'f_score': 0.8216276477146044},\n",
       "  'THREAT': {'tp': 0,\n",
       "   'fp': 0,\n",
       "   'fn': 150,\n",
       "   'tn': 52509,\n",
       "   'precision': 1,\n",
       "   'recall': 0.0,\n",
       "   'f_score': 0.0},\n",
       "  'INSULT': {'tp': 1884,\n",
       "   'fp': 691,\n",
       "   'fn': 690,\n",
       "   'tn': 49394,\n",
       "   'precision': 0.7316504854368931,\n",
       "   'recall': 0.7319347319347319,\n",
       "   'f_score': 0.7317925810837055},\n",
       "  'IDENTITY_HATE': {'tp': 2,\n",
       "   'fp': 2,\n",
       "   'fn': 475,\n",
       "   'tn': 52180,\n",
       "   'precision': 0.5,\n",
       "   'recall': 0.0041928721174004195,\n",
       "   'f_score': 0.008316008316008316},\n",
       "  'TEXTCAT_LOSSES': 3.300574591791758},\n",
       " {'TOXIC': {'tp': 3977,\n",
       "   'fp': 743,\n",
       "   'fn': 1070,\n",
       "   'tn': 46869,\n",
       "   'precision': 0.8425847457627119,\n",
       "   'recall': 0.7879928670497325,\n",
       "   'f_score': 0.8143749360090098},\n",
       "  'SEVERE_TOXIC': {'tp': 95,\n",
       "   'fp': 70,\n",
       "   'fn': 422,\n",
       "   'tn': 52072,\n",
       "   'precision': 0.5757575757575758,\n",
       "   'recall': 0.18375241779497098,\n",
       "   'f_score': 0.2785923753665689},\n",
       "  'OBSCENE': {'tp': 2283,\n",
       "   'fp': 495,\n",
       "   'fn': 479,\n",
       "   'tn': 49402,\n",
       "   'precision': 0.8218142548596112,\n",
       "   'recall': 0.8265749456915279,\n",
       "   'f_score': 0.824187725631769},\n",
       "  'THREAT': {'tp': 0,\n",
       "   'fp': 0,\n",
       "   'fn': 150,\n",
       "   'tn': 52509,\n",
       "   'precision': 1,\n",
       "   'recall': 0.0,\n",
       "   'f_score': 0.0},\n",
       "  'INSULT': {'tp': 1898,\n",
       "   'fp': 703,\n",
       "   'fn': 676,\n",
       "   'tn': 49382,\n",
       "   'precision': 0.7297193387158785,\n",
       "   'recall': 0.7373737373737373,\n",
       "   'f_score': 0.7335265700483092},\n",
       "  'IDENTITY_HATE': {'tp': 27,\n",
       "   'fp': 7,\n",
       "   'fn': 450,\n",
       "   'tn': 52175,\n",
       "   'precision': 0.7941176470588235,\n",
       "   'recall': 0.05660377358490566,\n",
       "   'f_score': 0.10567514677103719},\n",
       "  'TEXTCAT_LOSSES': 3.2344222257217226},\n",
       " {'TOXIC': {'tp': 3921,\n",
       "   'fp': 660,\n",
       "   'fn': 1126,\n",
       "   'tn': 46952,\n",
       "   'precision': 0.8559266535690897,\n",
       "   'recall': 0.7768971666336437,\n",
       "   'f_score': 0.8144993768176152},\n",
       "  'SEVERE_TOXIC': {'tp': 90,\n",
       "   'fp': 66,\n",
       "   'fn': 427,\n",
       "   'tn': 52076,\n",
       "   'precision': 0.5769230769230769,\n",
       "   'recall': 0.17408123791102514,\n",
       "   'f_score': 0.2674591381872214},\n",
       "  'OBSCENE': {'tp': 2279,\n",
       "   'fp': 429,\n",
       "   'fn': 483,\n",
       "   'tn': 49468,\n",
       "   'precision': 0.8415805022156573,\n",
       "   'recall': 0.8251267197682839,\n",
       "   'f_score': 0.8332723948811701},\n",
       "  'THREAT': {'tp': 1,\n",
       "   'fp': 0,\n",
       "   'fn': 149,\n",
       "   'tn': 52509,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.006666666666666667,\n",
       "   'f_score': 0.013245033112582783},\n",
       "  'INSULT': {'tp': 1846,\n",
       "   'fp': 644,\n",
       "   'fn': 728,\n",
       "   'tn': 49441,\n",
       "   'precision': 0.7413654618473896,\n",
       "   'recall': 0.7171717171717171,\n",
       "   'f_score': 0.7290679304897314},\n",
       "  'IDENTITY_HATE': {'tp': 84,\n",
       "   'fp': 37,\n",
       "   'fn': 393,\n",
       "   'tn': 52145,\n",
       "   'precision': 0.6942148760330579,\n",
       "   'recall': 0.1761006289308176,\n",
       "   'f_score': 0.28093645484949836},\n",
       "  'TEXTCAT_LOSSES': 2.8232882740304106},\n",
       " {'TOXIC': {'tp': 3930,\n",
       "   'fp': 690,\n",
       "   'fn': 1117,\n",
       "   'tn': 46922,\n",
       "   'precision': 0.8506493506493507,\n",
       "   'recall': 0.7786804042005152,\n",
       "   'f_score': 0.8130754111927175},\n",
       "  'SEVERE_TOXIC': {'tp': 107,\n",
       "   'fp': 86,\n",
       "   'fn': 410,\n",
       "   'tn': 52056,\n",
       "   'precision': 0.5544041450777202,\n",
       "   'recall': 0.20696324951644102,\n",
       "   'f_score': 0.30140845070422534},\n",
       "  'OBSCENE': {'tp': 2302,\n",
       "   'fp': 449,\n",
       "   'fn': 460,\n",
       "   'tn': 49448,\n",
       "   'precision': 0.836786623046165,\n",
       "   'recall': 0.833454018826937,\n",
       "   'f_score': 0.8351169961908218},\n",
       "  'THREAT': {'tp': 1,\n",
       "   'fp': 2,\n",
       "   'fn': 149,\n",
       "   'tn': 52507,\n",
       "   'precision': 0.3333333333333333,\n",
       "   'recall': 0.006666666666666667,\n",
       "   'f_score': 0.013071895424836602},\n",
       "  'INSULT': {'tp': 1863,\n",
       "   'fp': 639,\n",
       "   'fn': 711,\n",
       "   'tn': 49446,\n",
       "   'precision': 0.7446043165467626,\n",
       "   'recall': 0.7237762237762237,\n",
       "   'f_score': 0.7340425531914893},\n",
       "  'IDENTITY_HATE': {'tp': 124,\n",
       "   'fp': 49,\n",
       "   'fn': 353,\n",
       "   'tn': 52133,\n",
       "   'precision': 0.7167630057803468,\n",
       "   'recall': 0.259958071278826,\n",
       "   'f_score': 0.3815384615384615},\n",
       "  'TEXTCAT_LOSSES': 2.6497837365776666},\n",
       " {'TOXIC': {'tp': 3944,\n",
       "   'fp': 720,\n",
       "   'fn': 1103,\n",
       "   'tn': 46892,\n",
       "   'precision': 0.8456260720411664,\n",
       "   'recall': 0.7814543293045374,\n",
       "   'f_score': 0.8122747399855834},\n",
       "  'SEVERE_TOXIC': {'tp': 123,\n",
       "   'fp': 101,\n",
       "   'fn': 394,\n",
       "   'tn': 52041,\n",
       "   'precision': 0.5491071428571429,\n",
       "   'recall': 0.2379110251450677,\n",
       "   'f_score': 0.3319838056680162},\n",
       "  'OBSCENE': {'tp': 2309,\n",
       "   'fp': 469,\n",
       "   'fn': 453,\n",
       "   'tn': 49428,\n",
       "   'precision': 0.8311735061195105,\n",
       "   'recall': 0.835988414192614,\n",
       "   'f_score': 0.8335740072202167},\n",
       "  'THREAT': {'tp': 2,\n",
       "   'fp': 2,\n",
       "   'fn': 148,\n",
       "   'tn': 52507,\n",
       "   'precision': 0.5,\n",
       "   'recall': 0.013333333333333334,\n",
       "   'f_score': 0.025974025974025976},\n",
       "  'INSULT': {'tp': 1852,\n",
       "   'fp': 643,\n",
       "   'fn': 722,\n",
       "   'tn': 49442,\n",
       "   'precision': 0.7422845691382766,\n",
       "   'recall': 0.7195027195027195,\n",
       "   'f_score': 0.7307161175774314},\n",
       "  'IDENTITY_HATE': {'tp': 139,\n",
       "   'fp': 49,\n",
       "   'fn': 338,\n",
       "   'tn': 52133,\n",
       "   'precision': 0.7393617021276596,\n",
       "   'recall': 0.2914046121593291,\n",
       "   'f_score': 0.41804511278195483},\n",
       "  'TEXTCAT_LOSSES': 2.3880158715215387}]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INSULT_epoch1</th>\n",
       "      <th>TOXIC_epoch1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tp</th>\n",
       "      <td>1870.000000</td>\n",
       "      <td>3953.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fp</th>\n",
       "      <td>814.000000</td>\n",
       "      <td>957.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fn</th>\n",
       "      <td>704.000000</td>\n",
       "      <td>1094.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tn</th>\n",
       "      <td>49271.000000</td>\n",
       "      <td>46655.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.696721</td>\n",
       "      <td>0.805092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.726496</td>\n",
       "      <td>0.783238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_score</th>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.794014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           INSULT_epoch1  TOXIC_epoch1\n",
       "tp           1870.000000   3953.000000\n",
       "fp            814.000000    957.000000\n",
       "fn            704.000000   1094.000000\n",
       "tn          49271.000000  46655.000000\n",
       "precision       0.696721      0.805092\n",
       "recall          0.726496      0.783238\n",
       "f_score         0.711297      0.794014"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_df = pd.DataFrame.from_dict(evals_by_cat['INSULT'], orient='index', columns=['INSULT_epoch1'])\n",
    "second_df = pd.DataFrame.from_dict(evals_by_cat['TOXIC'], orient='index', columns=['TOXIC_epoch1'])\n",
    "\n",
    "pd.concat([first_df, second_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOXIC</th>\n",
       "      <th>SEVERE_TOXIC</th>\n",
       "      <th>OBSCENE</th>\n",
       "      <th>THREAT</th>\n",
       "      <th>INSULT</th>\n",
       "      <th>IDENTITY_HATE</th>\n",
       "      <th>TEXTCAT_LOSSES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tp</th>\n",
       "      <td>3953.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>2272.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1870.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>1.392019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fp</th>\n",
       "      <td>957.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>814.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>1.392019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fn</th>\n",
       "      <td>1094.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>1.392019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tn</th>\n",
       "      <td>46655.000000</td>\n",
       "      <td>51977.000000</td>\n",
       "      <td>49319.000000</td>\n",
       "      <td>52493.000000</td>\n",
       "      <td>49271.000000</td>\n",
       "      <td>52064.000000</td>\n",
       "      <td>1.392019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.805092</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>0.797193</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.696721</td>\n",
       "      <td>0.607973</td>\n",
       "      <td>1.392019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.783238</td>\n",
       "      <td>0.330754</td>\n",
       "      <td>0.822592</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.726496</td>\n",
       "      <td>0.383648</td>\n",
       "      <td>1.392019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_score</th>\n",
       "      <td>0.794014</td>\n",
       "      <td>0.400938</td>\n",
       "      <td>0.809694</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.470437</td>\n",
       "      <td>1.392019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  TOXIC  SEVERE_TOXIC       OBSCENE        THREAT  \\\n",
       "tp          3953.000000    171.000000   2272.000000     18.000000   \n",
       "fp           957.000000    165.000000    578.000000     16.000000   \n",
       "fn          1094.000000    346.000000    490.000000    132.000000   \n",
       "tn         46655.000000  51977.000000  49319.000000  52493.000000   \n",
       "precision      0.805092      0.508929      0.797193      0.529412   \n",
       "recall         0.783238      0.330754      0.822592      0.120000   \n",
       "f_score        0.794014      0.400938      0.809694      0.195652   \n",
       "\n",
       "                 INSULT  IDENTITY_HATE  TEXTCAT_LOSSES  \n",
       "tp          1870.000000     183.000000        1.392019  \n",
       "fp           814.000000     118.000000        1.392019  \n",
       "fn           704.000000     294.000000        1.392019  \n",
       "tn         49271.000000   52064.000000        1.392019  \n",
       "precision      0.696721       0.607973        1.392019  \n",
       "recall         0.726496       0.383648        1.392019  \n",
       "f_score        0.711297       0.470437        1.392019  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals_df = pd.DataFrame.from_dict(evals_by_cat)\n",
    "evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_text = dev_texts[11]\n",
    "# one_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Ok, let me say it again Come on, now you guys are just being piece of shit jews. I mean you have to admit, the guys in pink floyd play their instruments about as slow as a nigger works. I shouldn't even call what they play music. It's just a bunch of alarm clocks and cashier regirsters! But you know what the most pretentious thing about them is, its their lyrics. All af their songs are just surrealist poetry sung over doom-noise pop, and everyone starts calling them genius's over it. The truth is, their songs have no meaning. Take the album 'The Wall' for instance; sure it tells a story, but what is the moral and the meaning of the story? And dont tell me that the purpose of their songs is to make you think. The only way that music as slow as pink floyd could make you fucking think is if you were just as stoned as they are, which you kikes probly are... And one last time: 1) pink floyd fucking sucks 2) david fuckmor should taste my ass 3) you should to'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TOXIC': 0.9091559052467346,\n",
       " 'SEVERE_TOXIC': 0.0608687661588192,\n",
       " 'OBSCENE': 0.7635753154754639,\n",
       " 'THREAT': 0.007229546085000038,\n",
       " 'INSULT': 0.5386186242103577,\n",
       " 'IDENTITY_HATE': 0.08038196712732315}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(one_text)\n",
    "\n",
    "doc=nlp(one_text)\n",
    "doc.cats \n",
    "\n",
    "# sentiment_sum = sum([i.sentiment for i in doc])\n",
    "# print(sentiment_sum)\n",
    "# doc[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_config.cfg       \u001b[1m\u001b[36mspacy_multi_cat_model\u001b[m\u001b[m\n",
      "base_config.cfg       \u001b[1m\u001b[36mspacy_multi_cat_model\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls ../models\n",
    "nlp.to_disk(\"../models/spacy_multi_cat_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do 20 Additional Training Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model.\n",
      "Iterations: 20\n",
      "Timer Begins:20204.43\n",
      "epoch 1 start time: 20204.43\n",
      "textcat losses: 2.420857442188398\n",
      "epoch 1 end: 20904.773435177, epoch elapsed: 700.34\n",
      "\n",
      "epoch 2 start time: 20904.78\n",
      "textcat losses: 2.3911622935770898\n",
      "epoch 2 end: 21525.821319783, epoch elapsed: 621.05\n",
      "\n",
      "epoch 3 start time: 21525.82\n",
      "textcat losses: 2.1606388487485946\n",
      "epoch 3 end: 22142.51697684, epoch elapsed: 616.69\n",
      "\n",
      "epoch 4 start time: 22142.52\n",
      "textcat losses: 2.1463427109298654\n",
      "epoch 4 end: 22758.429444661, epoch elapsed: 615.91\n",
      "\n",
      "epoch 5 start time: 22758.43\n",
      "textcat losses: 1.9554609553060178\n",
      "epoch 5 end: 23372.526017913, epoch elapsed: 614.09\n",
      "\n",
      "epoch 6 start time: 23372.53\n",
      "textcat losses: 1.7881683352414486\n",
      "epoch 6 end: 23989.573797575, epoch elapsed: 617.05\n",
      "\n",
      "epoch 7 start time: 23989.58\n",
      "textcat losses: 1.7377497433548796\n",
      "epoch 7 end: 24608.659860403, epoch elapsed: 619.08\n",
      "\n",
      "epoch 8 start time: 24608.66\n",
      "textcat losses: 1.8173315413520463\n",
      "epoch 8 end: 25227.00988865, epoch elapsed: 618.35\n",
      "\n",
      "epoch 9 start time: 25227.01\n",
      "textcat losses: 1.6956643139270542\n",
      "epoch 9 end: 25847.205889942, epoch elapsed: 620.19\n",
      "\n",
      "epoch 10 start time: 25847.21\n",
      "textcat losses: 1.7224541110202012\n",
      "epoch 10 end: 26467.539332124, epoch elapsed: 620.33\n",
      "\n",
      "epoch 11 start time: 26467.54\n",
      "textcat losses: 1.6601265185052554\n",
      "epoch 11 end: 27090.641519992, epoch elapsed: 623.10\n",
      "\n",
      "epoch 12 start time: 27090.64\n",
      "textcat losses: 1.4837429331128997\n",
      "epoch 12 end: 27713.635978558, epoch elapsed: 622.99\n",
      "\n",
      "epoch 13 start time: 27713.64\n",
      "textcat losses: 1.6014918107316891\n",
      "epoch 13 end: 28337.734512396, epoch elapsed: 624.10\n",
      "\n",
      "epoch 14 start time: 28337.74\n",
      "textcat losses: 1.5043028015248396\n",
      "epoch 14 end: 28962.398363662, epoch elapsed: 624.66\n",
      "\n",
      "epoch 15 start time: 28962.40\n",
      "textcat losses: 1.465658484495704\n",
      "epoch 15 end: 29586.374418569, epoch elapsed: 623.97\n",
      "\n",
      "epoch 16 start time: 29586.38\n",
      "textcat losses: 1.4931953026266038\n",
      "epoch 16 end: 30211.187708347, epoch elapsed: 624.81\n",
      "\n",
      "epoch 17 start time: 30211.19\n",
      "textcat losses: 1.3733554731628068\n",
      "epoch 17 end: 30837.731921287, epoch elapsed: 626.54\n",
      "\n",
      "epoch 18 start time: 30837.73\n",
      "textcat losses: 1.5019921596112038\n",
      "epoch 18 end: 31461.526374207, epoch elapsed: 623.79\n",
      "\n",
      "epoch 19 start time: 31461.53\n",
      "textcat losses: 1.3984449250070798\n",
      "epoch 19 end: 32085.200269742, epoch elapsed: 623.67\n",
      "\n",
      "epoch 20 start time: 32085.20\n",
      "textcat losses: 1.3920188053328426\n",
      "epoch 20 end: 32709.921195399, epoch elapsed: 624.72\n",
      "\n",
      "Training complete. End:32709.92 Training Elapsed: 12505.49\n",
      "\n",
      "CPU times: user 6h 12min 7s, sys: 4h 1min, total: 10h 13min 8s\n",
      "Wall time: 3h 28min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log_list = list()\n",
    "\n",
    "# start = timer()\n",
    "# # ...\n",
    "# end = timer()\n",
    "# print(end - start) # Time in seconds, e.g. 5.38091952400282\n",
    "\n",
    "#(\"Number of training iterations\", \"n\", int))\n",
    "n_iter=20\n",
    "\n",
    "# Disabling other components\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  \n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    start_cumulative = timer()\n",
    "    print(f\"Training the model.\\nIterations: {n_iter}\\nTimer Begins:{start_cumulative:.2f}\")\n",
    "#     print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "\n",
    "    # Performing training\n",
    "    for i in range(n_iter):\n",
    "        start_epoch = timer()\n",
    "        print(f'epoch {i + 1} start time: {start_epoch:.2f}')\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        \n",
    "        for batch in batches:\n",
    "#             print(f\"new batch at :{timer():.2f}\")\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, \n",
    "                       annotations, \n",
    "                       sgd=optimizer, \n",
    "                       drop=0.2,\n",
    "                       losses=losses)\n",
    "\n",
    "      # Calling the evaluate() function and printing the scores\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            evals_by_cat = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            # reduced scores for testing\n",
    "            print(f'textcat losses:', losses['textcat'])\n",
    "            log_list.append(evals_by_cat)\n",
    "            \n",
    "            end_epoch = timer()\n",
    "            time.strftime(\"%Hh%Mm%Ss\", time.gmtime(4*3600+13*60+6)) \n",
    "            print(f'epoch {i + 1} end: {end_epoch}, epoch elapsed: {end_epoch-start_epoch:.2f}\\n')\n",
    "         \n",
    "            \n",
    "#             for key in evals_by_cat.keys():\n",
    "#                 print(f'{key}:\\n{evals_by_cat[key]}')\n",
    "#         print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  \n",
    "#               .format(losses['textcat'], scores['textcat_p'],\n",
    "#                       scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "end_cumulative = timer()\n",
    "print(f\"Training complete. End:{end_cumulative:.2f} Training Elapsed: {end_cumulative - start_cumulative:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preserve log list\n",
    "\n",
    "These logs are currently saved as dictionaries. We'll preserve them so we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../logs/log_list_2.pkl', 'wb') as f:\n",
    "    pickle.dump(log_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preserve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_config.cfg       \u001b[1m\u001b[36mspacy_multi_cat_model\u001b[m\u001b[m\n",
      "base_config.cfg       \u001b[1m\u001b[36mspacy_multi_cat_model\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls ../models\n",
    "config = nlp.config\n",
    "nlp.to_disk(\"../models/spacy_multi_cat_model/\")\n",
    "! ls ../models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Ok, let me say it again Come on, now you guys are just being piece of shit jews. I mean you have t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TOXIC': 0.9849706292152405,\n",
       " 'SEVERE_TOXIC': 0.007135968655347824,\n",
       " 'OBSCENE': 0.9465410113334656,\n",
       " 'THREAT': 0.007597020361572504,\n",
       " 'INSULT': 0.9003435373306274,\n",
       " 'IDENTITY_HATE': 0.7753530144691467}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test after 25 epochs\n",
    "print(one_text[:100])\n",
    "\n",
    "doc=nlp(one_text)\n",
    "doc.cats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notes to self\n",
    "\n",
    "https://v2.spacy.io/usage/processing-pipelines#pipelines\n",
    "https://v2.spacy.io/usage/processing-pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[expanding contractions](https://gist.github.com/widiger-anna/deefac010da426911381c118a97fc23f) \n",
    "[contractions](https://theslaps.medium.com/cant-stand-don-t-want-contractions-with-spacy-39715cac2ebb)  \n",
    "\n",
    "\n",
    "[text wrangling](https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html)  \n",
    "\n",
    "\n",
    "[nlp nltk vs spacy](https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/)  \n",
    "\n",
    "[pytorch](https://pytorch.org/https://pytorch.org/)  \n",
    "\n",
    "[text classification in python with spacy (try this one!)](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/)  \n",
    "\n",
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOCF+j+08tL5QgwPVB141nM",
   "collapsed_sections": [],
   "name": "toxic_text.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
