{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaWoMJGfVwUB"
   },
   "source": [
    "# Detecting and Classifying Toxic Comments\n",
    "# Part 2: Prediction with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why spaCy?\n",
    "\n",
    "Non-distructive analysis is part of spaCy's appeal. Text processed by spaCy is stored in a ```Doc``` object, from which [the original input text can always be reconstructed](https://stackoverflow.com/a/54617733).  \n",
    "\n",
    "spaCy's model pipelines also offer text categorization features for both binary and multivariate classification.\n",
    "\n",
    "We'll be using spaCy version 2.3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDRcsr5AZfQb",
    "tags": []
   },
   "source": [
    "## Python Library Imports\n",
    "\n",
    "\n",
    "Resources:\n",
    "- [pool]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24997,
     "status": "ok",
     "timestamp": 1616376286409,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "mQY7o6xDZhTe",
    "outputId": "73aee6d3-7edd-437f-eab4-36c67320b567",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "\n",
    "# scikit learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# tqdm & time\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTRgdYMCHaHD",
    "tags": []
   },
   "source": [
    "## spaCy Setup & Imports\n",
    "\n",
    "As mentioned previously, we'll be using spaCy version 2.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    2.3.5                         \n",
      "Location         /opt/anaconda3/lib/python3.7/site-packages/spacy\n",
      "Platform         Darwin-20.3.0-x86_64-i386-64bit\n",
      "Python version   3.7.6                         \n",
      "Models                                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # check version\n",
    "! python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy Imports\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "from spacy.scorer import Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from toxic_basic Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 77.5 ms, sys: 59 ms, total: 137 ms\n",
      "Wall time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "last load time:\n",
    "\n",
    "CPU times: user 67 ms, sys: 46.7 ms, total: 114 ms\n",
    "Wall time: 114 ms\n",
    "'''\n",
    "\n",
    "# load toxic_basic pickle into dataframe\n",
    "path_toxic_basic = \"../data/toxic_basic.pkl\"\n",
    "\n",
    "toxic_df = pd.read_pickle(path_toxic_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   comment_text          159571 non-null  object \n",
      " 1   uppercase_proportion  159548 non-null  float64\n",
      " 2   toxic                 159571 non-null  int64  \n",
      " 3   severe_toxic          159571 non-null  int64  \n",
      " 4   obscene               159571 non-null  int64  \n",
      " 5   threat                159571 non-null  int64  \n",
      " 6   insult                159571 non-null  int64  \n",
      " 7   identity_hate         159571 non-null  int64  \n",
      "dtypes: float64(1), int64(6), object(1)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training text and training outcomes into a list of tuples\n",
    "\n",
    "toxic_df[\"tuples\"] = toxic_df.apply(lambda row: (row['comment_text'], row['toxic']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.\",\n",
       " 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df['tuples'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_qN8SvmysyP"
   },
   "source": [
    "# Simple Train Test Split\n",
    "\n",
    "As our process should first determine whether the text is toxic or not toxic, we'll make a simplified stratified train test split, ensuring our balance of toxic and non toxic rows are proportionally distributed.\n",
    "\n",
    "For now, we won't be too concerned with the proportion of sub-categories, as our first step will be to filter not toxic from toxic, then run parallel operations for each toxic sub-category, as toxic sub-categories are not mutually exclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTncCacxukC-"
   },
   "source": [
    "## Stratified Split maintaining ratio of toxic to not toxic texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_text', 'uppercase_proportion', 'toxic', 'severe_toxic',\n",
       "       'obscene', 'threat', 'insult', 'identity_hate', 'tuples'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check current columns\n",
    "toxic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "aborted",
     "timestamp": 1616376571555,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "-3rJS3ZRuipN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X columns: Index(['comment_text', 'uppercase_proportion'], dtype='object')\n",
      "y columns:Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate',\n",
      "       'tuples'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# split df into X(independent) and y(depenendent) groups\n",
    "ind_cols = ['comment_text', 'uppercase_proportion']\n",
    "\n",
    "X = toxic_df[ind_cols]\n",
    "y = toxic_df.drop(columns=ind_cols)\n",
    "\n",
    "print(f\"X columns: {X.columns}\\ny columns:{y.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "aborted",
     "timestamp": 1616376571556,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "WJZnb8Pl1Stk"
   },
   "outputs": [],
   "source": [
    "# Train Test Split. Stratified on y['toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y['toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K Fold\n",
    "\n",
    "- [SKF docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "tiny_df = toxic_df.sample(20)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3,\n",
    "                      random_state=42,\n",
    "                      shuffle=True)\n",
    "print(skf)\n",
    "\n",
    "skf.get_n_splits(X_train['comment_text'], y_train['toxic'])\n",
    "\n",
    "train_indx, test_indx = next(skf.split(toxic_df['comment_text'], toxic_df['toxic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egZZdQA3hS2u"
   },
   "source": [
    "# spaCy\n",
    "\n",
    "Let's try out spaCy, a nlp processing library!\n",
    "\n",
    "- https://course.spacy.io/en/chapter1\n",
    "- [text classification with spaCy](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/) \n",
    "- [customized list of stopwords](https://spacy.io/usage/linguistic-features#stop-words)  \n",
    "- [Split Series into list of sentences](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.cat.html)  \n",
    "- [contractions](https://theslaps.medium.com/cant-stand-don-t-want-contractions-with-spacy-39715cac2ebb)  \n",
    "\n",
    "\n",
    "- [v2.spacy.io](https://v2.spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train spaCy Model for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4D7rYTjHXYY",
    "tags": []
   },
   "source": [
    "## Establish spaCy Pipeline\n",
    "\n",
    "\"spaCy's components are supervised models for text annotations, meaning that they can only learn to reproduce examples, not guess new labels from raw text.\"\n",
    "\n",
    "By default, spaCy's text categorizer is a simple convolutional neural network.\n",
    "\n",
    "Resources:\n",
    "- [for emojis](https://spacy.io/universe/project/spacymoji)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is modified from tutorial here:\n",
    "\n",
    "Resource:\n",
    "https://www.machinelearningplus.com/nlp/custom-text-classification-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "- [spaCy docs: scorer](https://spacy.io/api/scorer)  \n",
    "\n",
    "- [F-Score](https://en.wikipedia.org/wiki/F-score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## if the model is not yet locally available\n",
    "# ! python -m spacy download en_core_web_lg\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "# Provide scoring pipeline\n",
    "scorer = Scorer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger = nlp.create_pipe('tagger')\n",
    "textcat = nlp.create_pipe('textcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.add_pipe(tagger)\n",
    "nlp.add_pipe(textcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcat.add_label(\"TOXIC\")\n",
    "textcat.add_label(\"NOT TOXIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner', 'textcat']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I left off here!!!\n",
    "\n",
    "https://v2.spacy.io/usage/processing-pipelines#pipelines\n",
    "https://v2.spacy.io/usage/processing-pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.tokens import Doc\n",
    "# from spacy.training import Example\n",
    "\n",
    "\n",
    "def txt_and_cat(txt_series, cat_series):\n",
    "        \n",
    "    # convert each series or series slice to list\n",
    "    t = txt_series.tolist()\n",
    "    c = cat_series.tolist()\n",
    "    \n",
    "    # format categories\n",
    "    c = [{\"TOXIC\": bool(y), \"NOT TOXIC\": not bool(y)} for y in c]\n",
    "    c = [{'cats': i} for i in c]\n",
    "    \n",
    "    docs = list(zip(t, c))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Article for v3](https://medium.com/analytics-vidhya/building-a-text-classifier-with-spacy-3-0-dd16e9979a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! ls ../models/base_config.cfg\n",
    "# ! python -m spacy init fill-config ../models/base_config.cfg config.cfg --diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# formatting list of tuples for spacy training\n",
    "txt = toxic_df['comment_text'][train_indx]\n",
    "cat = toxic_df['toxic'][train_indx]\n",
    "\n",
    "train_docs = txt_and_cat(txt, cat)\n",
    "\n",
    "test_txt = toxic_df['comment_text'][test_indx]\n",
    "test_cat = toxic_df['toxic'][test_indx]\n",
    "\n",
    "test_docs = txt_and_cat(test_txt, test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106380 53191\n",
      "Explanation Why the edits made, {'cats': {'TOXIC': False, 'NOT TOXIC': True}}\n",
      "D'aww! He matches this backgro, {'cats': {'TOXIC': False, 'NOT TOXIC': True}}\n",
      "Hey man, I'm really not trying, {'cats': {'TOXIC': False, 'NOT TOXIC': True}}\n",
      "' More I can't make any real s, {'cats': {'TOXIC': False, 'NOT TOXIC': True}}\n",
      "' Congratulations from me as w, {'cats': {'TOXIC': False, 'NOT TOXIC': True}}\n"
     ]
    }
   ],
   "source": [
    "# this should be the correct format expected by the trainer\n",
    "print(len(train_docs), len(test_docs))\n",
    "\n",
    "# print(train_docs[0][1])\n",
    "first_five = [i for i in train_docs[:5]]\n",
    "\n",
    "for i in first_five:\n",
    "    print(f\"{i[0][:30]}, {i[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt_lst = [i[0] for i in test_docs]\n",
    "test_cat_lst = [i[1] for i in test_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/custom-text-classification-spacy\n",
    "\n",
    "# Not providing proper scoring..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if label == \"TOXIC\":\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.0\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.0\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_docs[:1000]\n",
    "dev_texts = test_txt_lst[:1000]\n",
    "dev_cats = test_cat_lst[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "0.836\t0.000\t0.000\t0.000\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "#(\"Number of training iterations\", \"n\", int))\n",
    "n_iter=1\n",
    "\n",
    "# Disabling other components\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  \n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "\n",
    "    # Performing training\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, \n",
    "                       annotations, \n",
    "                       sgd=optimizer, \n",
    "                       drop=0.2,\n",
    "                       losses=losses)\n",
    "\n",
    "      # Calling the evaluate() function and printing the scores\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  \n",
    "              .format(losses['textcat'], scores['textcat_p'],\n",
    "                      scores['textcat_r'], scores['textcat_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.8206   , -0.46732  ,  0.16149  , -0.10924  ,  0.43149  ,\n",
       "       -0.21921  ,  0.079141 , -0.58275  , -0.22628  ,  0.29776  ,\n",
       "        0.25014  , -0.083353 ,  0.44782  , -0.093115 ,  0.24839  ,\n",
       "        0.27882  ,  0.19327  , -0.85563  , -0.36409  , -0.34652  ,\n",
       "       -0.17957  , -0.1145   , -0.33472  ,  0.026779 , -0.0084624,\n",
       "       -0.23521  ,  0.36579  ,  0.068122 ,  0.35633  ,  0.34501  ,\n",
       "        0.73167  ,  0.33669  ,  0.19526  , -0.48853  ,  0.62343  ,\n",
       "       -0.28995  , -0.26048  , -0.71285  , -0.32637  ,  0.10947  ,\n",
       "       -0.13404  ,  0.2266   , -0.036429 , -0.35745  , -0.081528 ,\n",
       "       -0.0018481,  0.7685   ,  0.12117  ,  0.34245  ,  0.33979  ,\n",
       "       -0.22533  , -0.15521  , -0.33308  , -0.18812  , -0.38579  ,\n",
       "        0.35778  ,  0.23325  , -0.19504  ,  0.12371  , -0.78144  ,\n",
       "       -0.20727  , -0.20248  ,  0.062094 , -0.56965  , -0.16919  ,\n",
       "        0.1888   ,  0.05826  ,  0.093816 , -0.21475  ,  0.31174  ,\n",
       "        0.28475  ,  0.14408  , -0.40802  , -0.46086  , -0.14012  ,\n",
       "       -0.15025  , -0.41558  ,  0.15074  , -0.096331 ,  0.54255  ,\n",
       "       -0.10501  , -0.23871  , -0.059595 , -0.11017  , -0.24626  ,\n",
       "       -0.2432   ,  1.7754   ,  0.068556 , -0.13628  ,  0.22842  ,\n",
       "        0.023272 ,  0.43987  ,  0.16091  ,  0.016464 ,  0.16034  ,\n",
       "       -0.26476  ,  0.042968 , -1.1106   ,  0.0063511,  0.02328  ,\n",
       "        0.70617  ,  0.31332  , -0.014761 ,  0.21881  ,  0.59997  ,\n",
       "       -0.7517   ,  0.22499  ,  0.23745  , -0.41791  , -0.24105  ,\n",
       "       -0.15972  , -0.18336  , -0.11949  ,  0.42749  ,  0.20537  ,\n",
       "        0.27838  ,  0.013412 , -0.40986  , -0.3463   , -0.54156  ,\n",
       "       -0.096764 , -0.54199  , -0.035186 ,  0.27435  , -0.38043  ,\n",
       "        0.24993  , -0.31435  , -0.25629  , -0.55402  , -0.38151  ,\n",
       "       -0.25734  , -0.51887  ,  1.0585   ,  0.185    ,  0.34733  ,\n",
       "       -0.18998  ,  0.78947  , -0.01119  , -0.048271 ,  0.85549  ,\n",
       "       -1.7636   ,  0.2779   , -0.43153  , -0.62396  , -0.31742  ,\n",
       "       -0.58154  ,  0.34126  , -0.32849  , -0.22666  ,  0.13276  ,\n",
       "       -0.37195  ,  0.63368  , -0.15207  , -0.46359  ,  0.28076  ,\n",
       "        0.2301   ,  0.16337  , -0.42919  ,  0.24296  , -0.15824  ,\n",
       "       -0.27318  ,  0.27531  ,  0.20457  ,  0.15698  ,  0.056929 ,\n",
       "       -0.27813  , -0.018222 , -0.42392  , -0.49911  ,  0.10595  ,\n",
       "       -0.2738   , -0.27126  ,  0.39811  ,  0.083938 ,  0.35569  ,\n",
       "        0.26867  , -1.0901   ,  0.16498  , -0.70043  ,  0.2419   ,\n",
       "        0.55664  ,  0.43106  , -0.71402  ,  0.020279 , -0.11429  ,\n",
       "        0.099874 ,  0.3897   , -0.59684  ,  0.58199  ,  0.15149  ,\n",
       "        0.14309  ,  0.47495  ,  0.049333 , -0.32944  , -0.21021  ,\n",
       "       -0.099144 ,  0.39894  ,  0.61733  ,  0.23335  , -0.23802  ,\n",
       "       -0.4779   , -0.11398  , -0.27914  , -0.49535  , -0.06746  ,\n",
       "       -0.004717 ,  0.089377 ,  0.017923 , -0.23592  , -0.17749  ,\n",
       "       -0.84858  , -0.52183  , -0.12741  ,  0.9529   , -0.20968  ,\n",
       "        0.029385 ,  0.10631  , -0.17001  ,  0.38868  , -0.78473  ,\n",
       "       -0.38357  , -0.51036  ,  0.667    , -0.038766 ,  0.59147  ,\n",
       "       -0.18243  ,  0.0068823, -0.0758   ,  0.022004 ,  0.19821  ,\n",
       "       -0.11877  , -0.3034   , -0.1248   ,  0.48558  , -0.15453  ,\n",
       "        0.08348  ,  0.26851  , -0.13496  ,  0.4602   , -0.37977  ,\n",
       "        0.58458  , -0.26612  ,  0.29376  , -0.71159  ,  0.51811  ,\n",
       "        0.20718  , -0.3064   ,  0.24792  , -0.051926 ,  0.67313  ,\n",
       "       -0.81438  , -0.3177   ,  0.30747  , -0.50334  , -0.51641  ,\n",
       "       -0.12729  ,  0.13441  , -0.16656  ,  0.27448  , -0.68987  ,\n",
       "       -0.013374 , -0.14735  ,  0.35142  , -0.074817 ,  0.42062  ,\n",
       "        0.64698  , -0.63996  ,  0.010935 , -0.52097  ,  0.22285  ,\n",
       "       -0.66853  , -0.99386  , -0.80173  ,  0.31312  , -0.11872  ,\n",
       "       -0.58794  , -0.48062  ,  0.12551  ,  0.075167 ,  0.049682 ,\n",
       "       -0.32957  ,  0.38498  , -0.24102  ,  0.12913  , -0.15465  ,\n",
       "       -0.15326  ,  0.030055 ,  0.10006  , -0.20024  ,  0.22077  ,\n",
       "        0.54966  ,  0.15278  , -0.29382  , -0.85113  ,  0.26103  ,\n",
       "       -0.50596  ,  0.037978 ,  0.39672  ,  0.01762  , -0.69315  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model\n",
    "# txt = toxic_df['comment_text'][train_indx]\n",
    "# cat = toxic_df['toxic'][train_indx]\n",
    "\n",
    "# train_docs = txt_and_cat(txt, cat)\n",
    "\n",
    "# test_txt = toxic_df['comment_text'][test_indx]\n",
    "# test_cat = toxic_df['toxic'][test_indx]\n",
    "\n",
    "# test_docs = txt_and_cat(test_txt, test_cat)\n",
    "\n",
    "test_text = test_txt\n",
    "\n",
    "print(test_text[6])\n",
    "\n",
    "\n",
    "doc=nlp(test_text[6])\n",
    "doc.cats \n",
    "\n",
    "# sentiment_sum = sum([i.sentiment for i in doc])\n",
    "# print(sentiment_sum)\n",
    "# doc[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.\n",
      "{'TOXIC': 0.003213105956092477, 'NOT TOXIC': 0.9999545812606812}\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(train_docs[0][0])\n",
    "print(txt.iloc[0])\n",
    "print(doc.cats)\n",
    "\n",
    "# # doc.text\n",
    "# for i in doc:\n",
    "#     if i.is_alpha:\n",
    "#         print(i.lemma_.lower())\n",
    "\n",
    "def doc_check(tok):\n",
    "    '''\n",
    "    argument: doc.token\n",
    "    \n",
    "    checks for rejection conditions\n",
    "        not alpha\n",
    "        pronoun\n",
    "        stopword\n",
    "        \n",
    "    returns True if none are met\n",
    "    \n",
    "    ''' \n",
    "    # reject if not alpha\n",
    "    if tok.is_alpha == False:\n",
    "        return False\n",
    "    \n",
    "    # reject if pronoun\n",
    "    if tok.lemma_ == \"-PRON-\":\n",
    "        return False\n",
    "    \n",
    "    # reject if stopword\n",
    "    if tok.is_stop == True:\n",
    "        return False\n",
    "\n",
    "    # if not rejected, return true\n",
    "    return True\n",
    "\n",
    "lemmas_lc = [i.lemma_.lower() for i in doc if doc_check(i)]\n",
    "lemmas_lc\n",
    "\n",
    "sentiment_sum = sum([i.sentiment for i in doc])\n",
    "print(sentiment_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[expanding contractions](https://gist.github.com/widiger-anna/deefac010da426911381c118a97fc23f) \n",
    "[contractions](https://theslaps.medium.com/cant-stand-don-t-want-contractions-with-spacy-39715cac2ebb)  \n",
    "\n",
    "\n",
    "[text wrangling](https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html)  \n",
    "\n",
    "\n",
    "[nlp nltk vs spacy](https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/)  \n",
    "\n",
    "[pytorch](https://pytorch.org/https://pytorch.org/)  \n",
    "\n",
    "[text classification in python with spacy (try this one!)](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOCF+j+08tL5QgwPVB141nM",
   "collapsed_sections": [],
   "name": "toxic_text.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
