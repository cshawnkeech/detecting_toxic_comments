{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting and Classifying Toxic Comments\n",
    "# Part 3-1: TF*IDF & Random Forest Classifiers\n",
    "\n",
    "It may be possible to employ sequential binary models in order to get better results with rarer cases.\n",
    "\n",
    "If we first classify Toxic and Not Toxic, we could further process only the Toxic results against models that had been trained only to recognise sub-classes of toxic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# import custom trained spaCy model\n",
    "nlp = spacy.load(\"../models/spacy_2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# add src folder to path\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "# from text_prep import tidy_series, uppercase_proportion_column\n",
    "from spacy_helper import doc_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting info from preserved spaCy docs\n",
    "\n",
    "I've had a little difficulty with getting the doc properties to un-pickle and maintain the ability to further process them later. docs seem to depend on some vocab properties of the model that are not saved within the doc itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 39s, sys: 28.7 s, total: 4min 8s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "CPU times: user 3min 44s, sys: 33.5 s, total: 4min 17s\n",
    "Wall time: 4min 32s\n",
    "'''\n",
    "X_train = pd.read_pickle('../data/basic_df_split/X_train_2-1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_2-1.pkl         basic_X_test.pkl        basic_y_test.pkl\n",
      "X_train_docs_series.pkl basic_X_train.pkl       basic_y_train.pkl\n"
     ]
    }
   ],
   "source": [
    "# load y_train\n",
    "! ls ../data/basic_df_split/\n",
    "y_train = pd.read_pickle('../data/basic_df_split/basic_y_train.pkl')\n",
    "X_test = pd.read_pickle('../data/basic_df_split/basic_X_test.pkl')\n",
    "y_test = pd.read_pickle('../data/basic_df_split/basic_y_test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_text', 'uppercase_proportion', 'docs', 'lemmas', 'doc_vectors',\n",
       "       'tok_vectors'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of lemmas, less nltk stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.48 s, sys: 993 ms, total: 2.47 s\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove lemmas that appear in nltk stopword list\n",
    "X_train['lemmas_less'] = X_train['lemmas'].apply(lambda row: [lemma for lemma in row if lemma not in stopw_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reduce lemmas by min_length & max_length\n",
    "\n",
    "Exploration of corpus vocabulary suggests that lemmas of 2 or fewer characters are likely not very useful and can be removed to reduce features. \n",
    "\n",
    "Lemmas of longer than 20 characters are often run-on words (where spaces have been omitted). Although a few of them have words hidden within them that may be considered toxic, the rarity and non-standard format make them unlikely to be generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 663 ms, sys: 77.1 ms, total: 740 ms\n",
      "Wall time: 817 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "min_l = 3\n",
    "max_l = 20\n",
    "\n",
    "X_train['lemmas_less'] = X_train['lemmas'].apply(lambda row: [lemma for lemma in row if len(lemma) >= min_l or len(lemma) <= max_l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF*IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 s, sys: 1.19 s, total: 18.3 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_sklearn = TfidfVectorizer(ngram_range = (1,3),\n",
    "                                min_df = 2)\n",
    "\n",
    "# return sparse matrix\n",
    "# join list into individual strings\n",
    "tfidf_values = tfidf_sklearn.fit_transform(X_train['lemmas_less'].apply(lambda x: \" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<106912x459917 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4387497 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "last values (with min_df as 1):\n",
    "<106912x4089577 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 8002628 stored elements in Compressed Sparse Row format>\n",
    "'''\n",
    "\n",
    "tfidf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.86 s, sys: 4.86 s, total: 8.72 s\n",
      "Wall time: 9.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "CPU times: user 11.9 s, sys: 8.27 s, total: 20.2 s\n",
    "Wall time: 23 s\n",
    "'''\n",
    "lemmas_less_tfidf = pd.DataFrame(tfidf_values.toarray(),\n",
    "                                 columns=tfidf_sklearn.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106912, 459917)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_less_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106912,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['toxic'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(lemmas_less_tfidf['jerk'])\n",
    "\n",
    "# search_term = 'jerk'\n",
    "\n",
    "# # running this portion will crash the kernel\n",
    "# bool_mask = lemmas_less_tfidf.sort_values(search_term, ascending=False)[search_term][:10]\n",
    "# bool_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic: Random Forest Classifier\n",
    "\n",
    "Resources:\n",
    "- [Explanation of Warm Start for RFC (not what you may think)](https://stackoverflow.com/questions/42757892/how-to-use-warm-start/42763502)  \n",
    "- [TfidfVectorizer Docs](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_rfc = RandomForestClassifier(n_estimators=100,\n",
    "                                   max_depth = 10,\n",
    "                                   oob_score=True,\n",
    "                                   n_jobs=-1,\n",
    "                                   random_state=42,\n",
    "                                   warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lemmas_less_tfidf\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_logistic = LogisticRegression(warm_start=True,\n",
    "                                    random_state=42,\n",
    "                                    verbose=True,\n",
    "                                    solver='sag',\n",
    "                                    multi_class='ovr',\n",
    "                                    max_iter=100,\n",
    "                                    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1372\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m   1373\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "toxic_logistic.fit(X[:10],y['obscene'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(X, y, model, batch_size=1000, verbose=False, start=0):\n",
    "    \n",
    "    remaining = len(X)\n",
    "    \n",
    "    if batch_size > remaining:\n",
    "        bach_size = remaining\n",
    "    \n",
    "    b = start\n",
    "    e = batch_size\n",
    "    \n",
    "    while e <= remaining:\n",
    "        \n",
    "        model.fit(X[b:e], y[b:e])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_toxic = BernoulliNB\n",
    "# print(vect_X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(X_train.shape)\n",
    "\n",
    "# vect_X_train.iloc[0]\n",
    "\n",
    "# naive_toxic.fit(vect_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "# add src folder to path\n",
    "sys.path.insert(1, '../tokens')\n",
    "\n",
    "# from text_prep import tidy_series, uppercase_proportion_column\n",
    "from twitter_tok import twitter_api_key, api_secret_key, bearer_token, eml, password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# class ListStream:\n",
    "#     def __init__(self):\n",
    "#         self.data = []\n",
    "#     def write(self, s):\n",
    "#         self.data.append(s)\n",
    "\n",
    "# sys.stdout = x = ListStream()\n",
    "\n",
    "# for i in range(2):\n",
    "#     print ('i = ', i)\n",
    "\n",
    "# sys.stdout = sys.__stdout__\n",
    "# print(x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/master/Sampled-Stream/sampled-stream.py\n",
    "\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/sampled-stream/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.insert(1, '../tokens')\n",
    "from twitter_stream import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tweepy as tw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had little luck with get old tweets\n",
    "\n",
    "https://pypi.org/project/GetOldTweets3/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": [{\"id\": \"1377745160801562632\", \"value\": \"-is:retweet (place_country:US) lang:en\"}], \"meta\": {\"sent\": \"2021-04-01T22:28:52.615Z\"}}\n",
      "{\"meta\": {\"sent\": \"2021-04-01T22:28:53.935Z\", \"summary\": {\"deleted\": 1, \"not_deleted\": 0}}}\n",
      "{\"data\": [{\"value\": \"-is:retweet (place_country:US) lang:en\", \"id\": \"1377749842391351298\"}], \"meta\": {\"sent\": \"2021-04-01T22:28:55.380Z\", \"summary\": {\"created\": 1, \"not_created\": 0, \"valid\": 1, \"invalid\": 0}}}\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# twitter_rules.main()\n",
    "list_ = list()\n",
    "\n",
    "\n",
    "\n",
    "twitter_rules.main(text_list=list_, total_tweets=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\"There are, essentially, two Californias:\"',\n",
       " '\"@tesspro @StephenKing @GovRonDeSantis By the way, South Dakota is awesome. I\\\\u2019m not sure why you would, but it seems you may have been mocking where I live. Says the guy from the Inited States...\"',\n",
       " '\"It\\\\u2019s beginning\"',\n",
       " '\"Oh, yeah. This is why I shouldn\\\\u2019t watch so much Minnesota sports. It makes me feel bad. A loss stolen from the jaws of victory! #MNTwins\"',\n",
       " '\"@RichieWalsh 7 pitchers in one 9 inning game? Would\\'ve been 8 pitchers if Vasquez was eligible for work release.\"',\n",
       " '\"CNBC: Biden asks Education secretary to see if he can legally cancel student debt.\\\\n\\\\nC\\'mon man, let\\'s go! \\\\n\\\\nhttps://t.co/w5A9bkotqh\\\\n\\\\nvia @GoogleNews\"',\n",
       " '\"I would appreciate if everyone stopped attacking me for a tweet that was in the past\"',\n",
       " '\"And what https://t.co/kzpmaU5atl\"',\n",
       " '\"I could straight up murder a snowcone and nachos rn\"',\n",
       " '\"\\\\u201cThe crooks are going to get a gun!\\\\u201d said the guy who is okay making it a crime to give people waiting in line to vote a bottle of water. I mean, come on. The argument is \\\\u201cthe right to own this kind of gun is more important than the lives of the people it kills\\\\u201d, just own it. https://t.co/8ZKFHl7ZPx\"']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(list_))\n",
    "list_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- [Building Twitter Rules](https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
