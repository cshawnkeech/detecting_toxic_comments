{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaWoMJGfVwUB"
   },
   "source": [
    "# Toxic Text\n",
    "\n",
    "\n",
    "Detecting Insults in Social Commentary\n",
    "\n",
    "Data from Wikipedia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMY7K_Yx9NVJ"
   },
   "source": [
    "# Resources & Articles\n",
    "\n",
    "Resources:\n",
    "- [Detecting Insults in Social Commentary Dataset On Kaggle](https://www.kaggle.com/c/detecting-insults-in-social-commentary/data) \n",
    "- [Cleaned Toxic Comments on Kaggle](https://www.kaggle.com/fizzbuzz/cleaned-toxic-comments)  \n",
    "- [Insult Sets](https://www.kaggle.com/rogier2012/insult-sets)  \n",
    "- [Wikipedia Talk Labels: Personal Attacks](https://datasetsearch.research.google.com/search?query=stalking%20text&docid=L2cvMTFqbnl5cWw0Xw%3D%3D) \n",
    "    -  [At Kaggle](https://datasetsearch.research.google.com/search?query=stalking%20text&docid=L2cvMTFqbnl5cWw0Xw%3D%3D)  \n",
    "- [Toxic Dataset](https://www.kaggle.com/ra2041/toxic-dataset)  \n",
    "- [Dataset for Mean Birds: Detecting Agression and Bullying on Twitter](https://zenodo.org/record/1184178) \n",
    "\n",
    "Articles: \n",
    "- [NLP AND MACHINE LEARNING TECHNIQUES TO DETECT\n",
    "ONLINE HARASSMENT...(has links to datasets)](https://dalspace.library.dal.ca/handle/10222/76331) \n",
    "- [Detecting Cyberbullying...](http://www.ijetsr.com/images/short_pdf/1517199597_1428-1435-oucip915_ijetsr.pdf) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kivyT28oYMWZ"
   },
   "source": [
    "# Setup\n",
    "\n",
    "We'll mount our Google Drive and import any necessary Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qK_rIp9YYD4"
   },
   "source": [
    "## Mount Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1616376261699,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "gvgsrbNQYIzZ",
    "outputId": "870642f4-1eee-451c-964d-37865419de6f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1616376262082,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "zS90Y8MyYyVe",
    "outputId": "c349b7e1-fcc1-4489-cb9b-406a59b6c77d"
   },
   "outputs": [],
   "source": [
    "# ! pwd\n",
    "# /content\n",
    "\n",
    "# ! ls /content/gdrive/MyDrive/'Colab Notebooks'/capstone_exploration/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqBZisuwa0X7"
   },
   "source": [
    "## Kaggle Setup & Imports\n",
    "\n",
    "We'll be using at least one Kaggle dataset.\n",
    "\n",
    "Resources: \n",
    "\n",
    "- [Downloading Datasets directly into Google Drive](https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1616376262083,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "hUnUWyqKa0zz"
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# This code has been commented out as\n",
    "# it is only necessary to run this once with your credentials.\n",
    "\n",
    "# credentials, however, seem to be stored on the local machine\n",
    "# '''\n",
    "\n",
    "# from google.colab import files\n",
    "# files.upload() #this will prompt you to update the json\n",
    "\n",
    "# !pip install -q kaggle\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !ls ~/.kaggle\n",
    "# !chmod 600 /root/.kaggle/kaggle.json  # set permission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1616376262084,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "hMo9xs_eQ_nr"
   },
   "outputs": [],
   "source": [
    "# ! ls gdrive/MyDrive/'Colab Notebooks'/capstone_exploration/data/toxic_comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1616376262084,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "N-nZKv4PM-vd"
   },
   "outputs": [],
   "source": [
    "# ! pwd\n",
    "\n",
    "# ! ls gdrive/MyDrive/'Colab Notebooks'/capstone_exploration/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1616376262085,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "CQ4bXWwkJbWD"
   },
   "outputs": [],
   "source": [
    "# ! kaggle competitions list -s jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "# ! kaggle competitions download -c jigsaw-toxic-comment-classification-challenge -p /content/gdrive/MyDrive/Colab\\ Notebooks/capstone_exploration/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTRgdYMCHaHD"
   },
   "source": [
    "## spaCy Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23067,
     "status": "ok",
     "timestamp": 1616376284471,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "59WyH44vAItf",
    "outputId": "781ffc29-4ec9-4a24-874a-f59557d6608d"
   },
   "outputs": [],
   "source": [
    "# # spaCy setup\n",
    "# # update install to > version 3\n",
    "\n",
    "# ! pip install -U spacy\n",
    "\n",
    "# ! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy Imports\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDRcsr5AZfQb"
   },
   "source": [
    "## Python Library Imports\n",
    "\n",
    "\n",
    "Resources:\n",
    "- [pool]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download nltk resources if missing\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24997,
     "status": "ok",
     "timestamp": 1616376286409,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "mQY7o6xDZhTe",
    "outputId": "73aee6d3-7edd-437f-eab4-36c67320b567"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# scikit learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLj78JbffsQ8"
   },
   "source": [
    "## Import Data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv\n"
     ]
    }
   ],
   "source": [
    "! ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 26950,
     "status": "ok",
     "timestamp": 1616376288364,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "UeyUW0XJRVGY"
   },
   "outputs": [],
   "source": [
    "# # Load original from csv\n",
    "\n",
    "# # path if using google colabs\n",
    "# # path = \"gdrive/MyDrive/Colab Notebooks/capstone_exploration/data/toxic_comment_data/train.csv\"\n",
    "\n",
    "# # local path\n",
    "# path = '../data/train.csv'\n",
    "\n",
    "# toxic_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2pbrgM5f0o6"
   },
   "source": [
    "# Basic Exploration\n",
    "\n",
    "Texts in the dataset are labeled by human users as either **Toxic** or **Not Toxic**. \n",
    "\n",
    "Toxic comments can be further categorized as displaying any combination of five subcategories. Toxic comments can belong to any of the subcategories, multiple subcategories, or no further subcategories.\n",
    "\n",
    "Subcategories:\n",
    "- Severely toxic\n",
    "- Obscene\n",
    "- Threat\n",
    "- Insult\n",
    "- Identity hate\n",
    "\n",
    "### Category Summary\n",
    "\n",
    "| Category            \t| Totals \t|\n",
    "|---------------------\t|-------:\t|\n",
    "| Not Toxic         \t| 144277 \t|\n",
    "| Toxic             \t|  15294 \t|\n",
    "| Toxic Subcategories \t|        \t|\n",
    "| Severely toxic      \t|   1595 \t|\n",
    "| Obscene             \t|   8449 \t|\n",
    "| Threat              \t|    478 \t|\n",
    "| Insult              \t|   7877 \t|\n",
    "| Identity hate       \t|   1405 \t|\n",
    "| Subcategories Total \t|  19804 \t|\n",
    "\n",
    "\n",
    "### Proportions\n",
    "\n",
    "About 10% of the comments in the dataset are considered Toxic.\n",
    "\n",
    "```\n",
    "Proportion of Not Toxic Comments in Dataset: 0.9041555169799024\n",
    "Proportion of Toxic Comments in Dataset: 0.09584448302009764\n",
    "```\n",
    "\n",
    "\n",
    "Resources:\n",
    "- [Table Generator](https://www.tablesgenerator.com/markdown_tables#)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26944,
     "status": "ok",
     "timestamp": 1616376288366,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "453r8KHXRsZh",
    "outputId": "899de06c-d6f1-44d5-a249-676f78ffd696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows labeled as Not Toxic: 144277\n",
      "Rows labeled as Toxic:      15294\n",
      "\n",
      "\n",
      "severe_toxic     1595\n",
      "obscene          8449\n",
      "threat            478\n",
      "insult           7877\n",
      "identity_hate    1405\n",
      "dtype: int64 \n",
      "\n",
      "total sub_toxic:            19804\n"
     ]
    }
   ],
   "source": [
    "# how many rows labeled as not toxic?\n",
    "not_toxic_count = toxic_df[toxic_df['toxic']==0].shape[0]\n",
    "print(f\"Rows labeled as Not Toxic: {not_toxic_count}\") # not toxic: (144277) \n",
    "\n",
    "# rows labeled toxic\n",
    "toxic_count = toxic_df[toxic_df['toxic']==1].shape[0]\n",
    "print(f\"Rows labeled as Toxic:      {toxic_count}\") # toxic: (15294)\n",
    "print('\\n')\n",
    "sub_toxic = toxic_df[['severe_toxic', 'obscene','threat','insult','identity_hate']].sum()\n",
    "\n",
    "print(sub_toxic, '\\n')\n",
    "print(f\"total sub_toxic:            {sub_toxic.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26936,
     "status": "ok",
     "timestamp": 1616376288367,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "7A-LuVRNnRfc",
    "outputId": "9caf5744-63cd-4cbb-e413-0e23a9f72106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Not Toxic Comments in Dataset: 0.9041555169799024\n",
      "Proportion of Toxic Comments in Dataset: 0.09584448302009764\n"
     ]
    }
   ],
   "source": [
    "# Proportions:\n",
    "total_rows = toxic_df.shape[0] # 159571\n",
    "\n",
    "# Not Toxic Proportion\n",
    "not_toxic_prop = not_toxic_count/total_rows # 0.9041555169799024\n",
    "print(f\"Proportion of Not Toxic Comments in Dataset: {not_toxic_prop}\")\n",
    "\n",
    "# Toxic Proportion\n",
    "toxic_prop = toxic_count/total_rows # 0.09584448302009764\n",
    "print(f\"Proportion of Toxic Comments in Dataset: {toxic_prop}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxhq-7EOfpsB"
   },
   "source": [
    "# Drop 'id' Column From Full Dataset\n",
    "The id column is not really useful for our purposes, so we'll drop it from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 26935,
     "status": "ok",
     "timestamp": 1616376288367,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "EXnAvoBef08q"
   },
   "outputs": [],
   "source": [
    "toxic_df.drop(columns='id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gr3jPH3xfJkg"
   },
   "source": [
    "# Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1Md0mKVKwF9"
   },
   "source": [
    "Cleaning Functions:\n",
    "- convert interior quotes to all single quotes\n",
    "- strip any extraneous whitespace\n",
    "- strip any ip addresses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 27061,
     "status": "ok",
     "timestamp": 1616376288495,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "-yt0qUU9aTpR"
   },
   "outputs": [],
   "source": [
    "# Convert all interior quotes to single quotes\n",
    "\n",
    "def convert_interior_quotes(s):\n",
    "    '''\n",
    "    Arguments:\n",
    "        s = Series of strings\n",
    "            Takes a series of strings as an argument\n",
    "            converts all interior quotes in a string to single quotes\n",
    "    Returns: \n",
    "        Series of strings with interior quotes\n",
    "    '''\n",
    "    quotes_pattern = '[\"]+'\n",
    "    return s.str.replace(quotes_pattern, \"'\")\n",
    "\n",
    "def strip_ip(s):\n",
    "    '''\n",
    "    Arguments:\n",
    "        s = Series of strings\n",
    "            Takes a series of strings as an argument\n",
    "            removes any ip addresses\n",
    "    Returns: \n",
    "        Series of strings without ip addresses\n",
    "    '''\n",
    "    ip_pat = '(?:[0-9]{1,3}\\.){3}[0-9]{1,3}'\n",
    "    return s.str.replace(ip_pat, \"\")\n",
    "\n",
    "def strip_whitespace(s):\n",
    "    '''\n",
    "    Arguments:\n",
    "        s = Series of strings\n",
    "            Takes a series of strings as an argument\n",
    "            removes extraneous whitespace\n",
    "    Returns: \n",
    "        Series of strings without extraneous whitespace\n",
    "    '''\n",
    "    \n",
    "    t = s.copy()\n",
    "    # remove whitespace from edge\n",
    "    t = t.str.strip()\n",
    "\n",
    "    # reduce interior whitespace to single space\n",
    "    t = t.str.replace('[\\s]+', ' ')\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def remove_all_punct(s):\n",
    "    '''\n",
    "    Arguments:\n",
    "        s = Series of strings\n",
    "            Takes a series of strings as an argument\n",
    "            removes all punctuation\n",
    "    Returns: \n",
    "        Series of strings with no punctuation\n",
    "    '''\n",
    "    not_alpha_pattern = '[^A-Za-z\\s]'\n",
    "    return s.str.replace(not_alpha_pattern, \"\")\n",
    "\n",
    "def tidy_series(s):\n",
    "    '''\n",
    "    returns tidied series\n",
    "    '''\n",
    "    # copy series\n",
    "    t = s.copy()\n",
    "\n",
    "    # call individual functions\n",
    "    t = convert_interior_quotes(t)\n",
    "    t = strip_whitespace(t)\n",
    "    t = strip_ip(t)\n",
    "\n",
    "    return t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6FBNzeFz_Z3"
   },
   "source": [
    "## Apply Cleaning to Full Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 35569,
     "status": "ok",
     "timestamp": 1616376297005,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "aIrrLup_eB8n"
   },
   "outputs": [],
   "source": [
    "# tidy comment_text\n",
    "toxic_df['comment_text'] = tidy_series(toxic_df['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfuWNkaLxJyq"
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "There are a few features that are not obvious in the original dataset that may be useful for prediction and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1wmz5_OHWd0"
   },
   "source": [
    "spaCy: doc and raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35944,
     "status": "ok",
     "timestamp": 1616376297407,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "1UW8V-VHOey0",
    "outputId": "99fd51ed-3b06-436e-960e-aaeabff523dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 53680,
     "status": "ok",
     "timestamp": 1616376315148,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "BsrgbZVIIJWq"
   },
   "outputs": [],
   "source": [
    "# doc1 = nlp(toxic_df['comment_text'][0])\n",
    "\n",
    "# for x, token in enumerate(doc1):\n",
    "#     print(x, token.lemma_)\n",
    "\n",
    "# textcat = nlp.add_pipe('textcat')\n",
    "# print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53673,
     "status": "ok",
     "timestamp": 1616376315149,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "Ww6Ee8ugFqLo",
    "outputId": "bf549b15-e82d-498d-d423-7041793ced37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def doc_per_row(s):\n",
    "\n",
    "    t = s.copy()\n",
    "    \n",
    "    t = remove_all_punct(t)\n",
    "    t = t.str.strip()\n",
    "\n",
    "    return t.apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rq450eCERmnE"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# toxic_df['doc_raw'] = doc_per_row(toxic_df['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL3Pg8V2kV9o"
   },
   "source": [
    "Resource:  \n",
    "- [running pandas operations in parallel](http://www.racketracer.com/2016/07/06/pandas-in-parallel/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1616377214101,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "18A-7gyNjUEF"
   },
   "outputs": [],
   "source": [
    "# # parallelize dataframe\n",
    "\n",
    "from multiprocessing import Pool\n",
    "# import multiprocessing\n",
    "\n",
    "# multiprocessing.cpu_count() # 2 for colabs\n",
    "# num_partitions = 100\n",
    "# num_cores = 4\n",
    "\n",
    "def parallelize_dataframe(df, func, num_cores=2, num_partitions=100):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dK6ddClhlY4_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 35s, sys: 4min 24s, total: 8min 59s\n",
      "Wall time: 18min 31s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Warning!\n",
    "\n",
    "Processing this cell takes about 20 min using 4 cores\n",
    "\n",
    "CPU times: user 4min 35s, sys: 4min 24s, total: 8min 59s\n",
    "Wall time: 18min 31s\n",
    "'''\n",
    "# # test = parallelize_dataframe(toxic_df['comment_text'][:20], doc_per_row)\n",
    "\n",
    "# %%time\n",
    "# toxic_df['doc_per_row'] = parallelize_dataframe(toxic_df['comment_text'], doc_per_row, num_cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv\n",
      "toxic_2-1 train.csv\n"
     ]
    }
   ],
   "source": [
    "# # pickle current columns, including new doc column\n",
    "'''\n",
    "Warning!\n",
    "\n",
    "Processing this cell takes about 10 min\n",
    "'''\n",
    "\n",
    "# ! ls ../data\n",
    "# toxic_df.to_pickle(\"../data/toxic_2-1.pkl\")\n",
    "\n",
    "# ! ls ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAEpNVei0Vib"
   },
   "source": [
    "classification with spacy\n",
    "\n",
    "- [Really useful article](https://www.machinelearningplus.com/nlp/custom-text-classification-spacy/)  \n",
    "- [another project on kaggle](https://www.kaggle.com/poonaml/text-classification-using-spacy)  \n",
    "- [turbo charge you spacy nlp pipeline](https://towardsdatascience.com/turbo-charge-your-spacy-nlp-pipeline-551435b664ad) \n",
    "- [python & spacy nlp](https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/)  \n",
    "- [cheat sheet](https://www.datacamp.com/community/blog/spacy-cheatsheet)  \n",
    "\n",
    "\n",
    "Other Resources:\n",
    "- [Naive bayes](https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html)  \n",
    "\n",
    "Video Resources:\n",
    "- [spacy introduction, tokenization, lemmatization, stemming & techniques](https://www.youtube.com/watch?v=ZIiho_JfJNw)  \n",
    "\n",
    "-\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 111,
     "status": "aborted",
     "timestamp": 1616376571548,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "94hgfYNqFp8s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def raw_lemma_per_row(s):\n",
    "\n",
    "    t = s.copy()\n",
    "    return t.apply(lambda x: [i.lemma_ for i in x])\n",
    "\n",
    "# test_lemma = raw_lemma_per_row(test)\n",
    "\n",
    "# print(test_lemma)\n",
    "# # raw_lemma_lst = [doc_v.lemma_ for doc_v in raw_doc_list]\n",
    "\n",
    "# # toxic_df['raw_lemma'] = pd.Series(raw_lemma_lst)\n",
    "\n",
    "# type(test_lemma)\n",
    "\n",
    "# print(toxic_df['comment_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 106,
     "status": "aborted",
     "timestamp": 1616376571549,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "PLkUsSJDV4mi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 24s, sys: 3min 38s, total: 7min 3s\n",
      "Wall time: 18min 27s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Warning!\n",
    "\n",
    "Processing this cell takes about 20 min using 4 cores\n",
    "\n",
    "CPU times: user 3min 24s, sys: 3min 38s, total: 7min 3s\n",
    "Wall time: 18min 27s\n",
    "'''\n",
    "\n",
    "# %%time\n",
    "# # # toxic_df['lemma_raw'] = raw_lemma_per_row(toxic_df['doc_raw'])\n",
    "# toxic_df['lemma_raw'] = parallelize_dataframe(toxic_df['doc_per_row'], raw_lemma_per_row, num_cores=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 103,
     "status": "aborted",
     "timestamp": 1616376571551,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "Rx7ZhmtkfNuX"
   },
   "outputs": [],
   "source": [
    "# toxic_df['lemma_raw'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 100,
     "status": "aborted",
     "timestamp": 1616376571552,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "eTXlBXqqfazt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 41s, sys: 3min 27s, total: 6min 9s\n",
      "Wall time: 16min 20s\n"
     ]
    }
   ],
   "source": [
    "# # Pickle version 2\n",
    "\n",
    "# %%time\n",
    "# # pickle current df\n",
    "# toxic_df.to_pickle(\"../data/toxic_2-2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 93,
     "status": "aborted",
     "timestamp": 1616376571552,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "g5F8LmQcPpBW"
   },
   "outputs": [],
   "source": [
    "# for token in doc1:\n",
    "# #   print(f'{token.text:{20}} {token.lemma_:{20}} {token.pos_:{10}}')\n",
    "#     print(f'{token.lemma_:{20}}')\n",
    "#     # print(f'{token.lemma_:{21}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from 2-2 Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_2-1.pkl toxic_2-2.pkl train.csv\n"
     ]
    }
   ],
   "source": [
    "! ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 1min 3s, total: 3min 54s\n",
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CPU times: user 2min 51s, sys: 1min 3s, total: 3min 54s\n",
    "Wall time: 4min 31s\n",
    "'''\n",
    "\n",
    "# %%time\n",
    "# # load from pickle file 2-2a\n",
    "# path2_2 = \"../data/toxic_2-2.pkl\"\n",
    "\n",
    "# toxic_df = pd.read_pickle(path2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hD5ZPqZ1g2E"
   },
   "source": [
    "## Proportion of All-Caps Type\n",
    "\n",
    "In many circles, typing in all caps is considered a way to indicate yelling. Before changing the initial text, we'll record the proportion of upper case letters to the total number of alphabetical characters. \n",
    "\n",
    "PossibleConfounds:\n",
    "- [People with dislexia occasionally choose all-caps as an accomodataion](https://www.readandspell.com/us/writing-in-all-caps)  \n",
    "- Quoted all-caps text\n",
    "    - not counting quoted and block quoted text may help here.\n",
    "- Text referencing all-caps acronymns\n",
    "- Programming language conventions\n",
    "    - e.g. SQL syntax typically inlcudes all-caps reserved words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqfBTb7-JF2k"
   },
   "source": [
    "### Custom Function: uppercase_proportion_column(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 89,
     "status": "aborted",
     "timestamp": 1616376571553,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "4z1KnAdZ0ApC"
   },
   "outputs": [],
   "source": [
    "def uppercase_proportion_column(s):\n",
    "    '''\n",
    "    given a pandas Series:\n",
    "        containing rows of strings\n",
    "    returns: a series of floats representing\n",
    "        the percentage of capital letters vs total alpha chars\n",
    "        in provided strings\n",
    "    '''\n",
    "    import re # dependent on re\n",
    "\n",
    "    uc_pattern = '[A-Z]'\n",
    "    alpha_pattern = '[A-Za-z]'\n",
    "\n",
    "    cap_count = s.str.findall(uc_pattern).str.len()\n",
    "    # print(cap_count)\n",
    "\n",
    "    alpha_char_count = s.str.findall(alpha_pattern).str.len()\n",
    "    # print(alpha_char_count)\n",
    "\n",
    "    uc_proportion = cap_count / alpha_char_count\n",
    "    # print(uc_proportion)\n",
    "\n",
    "    return uc_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 85,
     "status": "aborted",
     "timestamp": 1616376571553,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "0H2Buj2GSDU7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practice, practiced, practicing\n"
     ]
    }
   ],
   "source": [
    "doc_lemma = nlp('Practice, practiced, practicing')\n",
    "doc_lemma\n",
    "print(doc_lemma) # can be indexed as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "aborted",
     "timestamp": 1616376571554,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "Rv2kd0KgSLtj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practice practice practice\n",
      ", , ,\n",
      "practiced practice practice\n",
      ", , ,\n",
      "practicing practice practice\n"
     ]
    }
   ],
   "source": [
    "for token in doc_lemma:\n",
    "    print(token.text, token.lemma_, token.lemma_.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "aborted",
     "timestamp": 1616376571554,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "Kse7sDLwYp5P"
   },
   "outputs": [],
   "source": [
    "# short_df['uppercase_proportion'] = uppercase_proportion_column(short_df['comment_text'])\n",
    "# short_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sF18XirY0QoT"
   },
   "source": [
    "## Apply Custom Features to Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 72,
     "status": "aborted",
     "timestamp": 1616376571555,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "6bOQnYDswWCM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate', 'doc_per_row', 'lemma_raw', 'uppercase_proportion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create uppercase_proportion column\n",
    "toxic_df['uppercase_proportion'] = uppercase_proportion_column(toxic_df['comment_text'])\n",
    "toxic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 31s, sys: 2min 45s, total: 5min 16s\n",
      "Wall time: 12min 24s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# '''\n",
    "# CPU times: user 2min 31s, sys: 2min 45s, total: 5min 16s\n",
    "# Wall time: 12min 24s\n",
    "# '''\n",
    "# # # Pickle version 3\n",
    "# # toxic_df.to_pickle(\"../data/toxic_2-3.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from 2-3 Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 1min 9s, total: 4min 9s\n",
      "Wall time: 4min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNote:\\nlast load time (for version 2-2) was:\\n\\nCPU times: user 2min 51s, sys: 1min 3s, total: 3min 54s\\nWall time: 4min 31s\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# load from pickle file 2-3\n",
    "'''\n",
    "last load time:\n",
    "\n",
    "CPU times: user 2min 59s, sys: 1min 9s, total: 4min 9s\n",
    "Wall time: 4min 47s\n",
    "'''\n",
    "path2_3 = \"../data/toxic_2-3.pkl\"\n",
    "\n",
    "toxic_df = pd.read_pickle(path2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_qN8SvmysyP"
   },
   "source": [
    "# Simple Train Test Split\n",
    "\n",
    "As our process should first determine whether the text is toxic or not toxic, we'll make a simplified stratified train test split, ensuring our balance of toxic and non toxic rows are proportionally distributed.\n",
    "\n",
    "For now, we won't be too concerned with the proportion of sub-categories, as our first step will be to filter not toxic from toxic, then run parallel operations for each toxic sub-category, as toxic sub-categories are not mutually exclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTncCacxukC-"
   },
   "source": [
    "## Stratified Split maintaining ratio of toxic to not toxic texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate', 'doc_per_row', 'lemma_raw', 'uppercase_proportion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check current columns\n",
    "toxic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "aborted",
     "timestamp": 1616376571555,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "-3rJS3ZRuipN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X columns: Index(['comment_text', 'doc_per_row', 'lemma_raw', 'uppercase_proportion'], dtype='object')\n",
      "y columns:Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
      "       'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# split df into X(independent) and y(depenendent) groups\n",
    "ind_cols = ['comment_text', 'doc_per_row', 'lemma_raw', 'uppercase_proportion']\n",
    "\n",
    "X = toxic_df[ind_cols]\n",
    "y = toxic_df.drop(columns=ind_cols)\n",
    "\n",
    "print(f\"X columns: {X.columns}\\ny columns:{y.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "aborted",
     "timestamp": 1616376571556,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "WJZnb8Pl1Stk"
   },
   "outputs": [],
   "source": [
    "# Train Test Split. Stratified on y['toxic']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            comment_text  \\\n",
      "27301  ' Meša Selimović I'm not opposing such a formu...   \n",
      "\n",
      "                                             doc_per_row  \\\n",
      "27301  (Mea, Selimovi, I, m, not, opposing, such, a, ...   \n",
      "\n",
      "                                               lemma_raw  uppercase_proportion  \n",
      "27301  [Mea, Selimovi, I, m, not, oppose, such, a, fo...              0.045455  \n",
      "       toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "27301      0             0        0       0       0              0\n"
     ]
    }
   ],
   "source": [
    "tiny_X_train = X_train[0:5]\n",
    "\n",
    "tiny_y_train = y_train[0:5]\n",
    "\n",
    "print(tiny_X_train.head(1))\n",
    "print(tiny_y_train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06935782330899092\n",
      "0.060543219369827816\n",
      "0.1525068846306749\n"
     ]
    }
   ],
   "source": [
    "print(X_train['uppercase_proportion'].mean())\n",
    "print(X_train['uppercase_proportion'][y_train['toxic']==0].mean())\n",
    "print(X_train['uppercase_proportion'][y_train['toxic']==1].mean())\n",
    "\n",
    "'''\n",
    "For the training set\n",
    "\n",
    "mean all comments: 0.06935782330899092\n",
    "mean not toxic:    0.060543219369827816\n",
    "mean toxic:        0.1525068846306749\n",
    "'''\n",
    "# uppercase proportion for toxic comments is over twice that of not toxic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egZZdQA3hS2u"
   },
   "source": [
    "# spaCy\n",
    "\n",
    "Let's try out spaCy, a nlp processing library!\n",
    "\n",
    "- https://course.spacy.io/en/chapter1\n",
    "- [text classification with spaCy](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/) \n",
    "- [customized list of stopwords](https://spacy.io/usage/linguistic-features#stop-words)  \n",
    "- [Split Series into list of sentences](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.cat.html)  \n",
    "- [contractions](https://theslaps.medium.com/cant-stand-don-t-want-contractions-with-spacy-39715cac2ebb)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy Imports\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "aborted",
     "timestamp": 1616376571556,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "lVvUNZThCrZg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.0.5                         \n",
      "Location         /opt/anaconda3/lib/python3.7/site-packages/spacy\n",
      "Platform         Darwin-20.3.0-x86_64-i386-64bit\n",
      "Python version   3.7.6                         \n",
      "Pipelines        en_core_web_lg (3.0.0)        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # check version\n",
    "! python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4D7rYTjHXYY"
   },
   "source": [
    "## Establish spaCy Pipeline\n",
    "\n",
    "\"spaCy's components are supervised models for text annotations, meaning hey can only learn to reproduce examples, not guess new labels from raw text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0vUG21gj8a2K"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two models\n",
    "# # textcat: binary: toxic or not toxit\n",
    "nlp.add_pipe(\"textcat\", last=True)\n",
    "\n",
    "# # textcat_multilabel: what type of toxic if toxic?\n",
    "nlp.add_pipe(\"textcat_multilabel\", last=True)\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is modified from tutorial here:\n",
    "\n",
    "Resource:\n",
    "https://www.machinelearningplus.com/nlp/custom-text-classification-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model weights randomly \n",
    "# nlp.begin_training\n",
    "\n",
    "# format\n",
    "\n",
    "# test using small batch:\n",
    "\n",
    "ind_text = tiny_X_train['comment_text']\n",
    "dep_text = tiny_y_train['toxic']\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    train_data = zip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMNCTHA7oIMs"
   },
   "source": [
    "# NLTK Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "aborted",
     "timestamp": 1616376571558,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "j6Ad7j6gofEf"
   },
   "outputs": [],
   "source": [
    "# ! pip install --user -U nltk\n",
    "# ! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "aborted",
     "timestamp": 1616376571558,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "ZhjhN7nniACA"
   },
   "outputs": [],
   "source": [
    "# from nltk.classify import naivebayes\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "# classif = SklearnClassifier(LinearSVC())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                     ('chi2', SelectKBest(chi2, k=1000)),\n",
    "                     ('nb', MultinomialNB())])\n",
    "classif = SklearnClassifier(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "aborted",
     "timestamp": 1616376571559,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "b8w-SaNNqm8Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1616376571559,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "lBIF3S3aNgPt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcAdz69RyFvN"
   },
   "source": [
    "# SKLearn \n",
    "\n",
    "- Resources: \n",
    "[Naive Bayes Classification ](https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "aborted",
     "timestamp": 1616376571560,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "oZQZmkaAyD-A"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "aborted",
     "timestamp": 1616376571560,
     "user": {
      "displayName": "Shawn Keech",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicFJvHjGKSkP2elE1kc5WTSumC2FDDidD75Ssv0w=s64",
      "userId": "08670766559094446918"
     },
     "user_tz": 300
    },
    "id": "t8YAxtxGyeIi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter core     : 4.6.1\n",
      "jupyter-notebook : 6.0.3\n",
      "qtconsole        : 4.6.0\n",
      "ipython          : 7.12.0\n",
      "ipykernel        : 5.1.4\n",
      "jupyter client   : 5.3.4\n",
      "jupyter lab      : 1.2.6\n",
      "nbconvert        : 5.6.1\n",
      "ipywidgets       : 7.5.1\n",
      "nbformat         : 5.0.4\n",
      "traitlets        : 4.3.3\n"
     ]
    }
   ],
   "source": [
    "! jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOCF+j+08tL5QgwPVB141nM",
   "collapsed_sections": [],
   "name": "toxic_text.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
